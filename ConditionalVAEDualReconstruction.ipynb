{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE\n",
    "\n",
    "With Input:\n",
    "- Image Label\n",
    "- Coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        #image = image/(image.max()/255.0)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #print(shape_label)\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "#         print(cam_loc)\n",
    "        cam_loc = torch.tensor([cam_loc[1], cam_loc[2]])\n",
    "#         print(cam_loc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        self.img_lin1 = nn.Linear(init_filters*(conv_out_size**2), 1024)\n",
    "        self.img_lin2 = nn.Linear(4096, 1024)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(2,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(1024+16+6,256)\n",
    "\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.sigma = nn.Linear(256, z_dim)\n",
    "        \n",
    "    def forward(self, image, label, coord):\n",
    "        \n",
    "        #Image\n",
    "        x = self.conv1(image)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\" + str(x.shape))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"after flatten:\" + str(x.shape))\n",
    "\n",
    "        x = self.img_lin1(x)\n",
    "        x = F.relu(x)   \n",
    "\n",
    "        #Coordinate\n",
    "        coord = self.coord_lin1(coord)\n",
    "        \n",
    "        #Label\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "#         print(x.shape)\n",
    "#         print(label.shape)\n",
    "#         print(coord.shape)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x, label, coord],dim=1)\n",
    "#         print(concat.shape)\n",
    "        \n",
    "        x = self.comb_lin1(concat)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        \n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(z_dim+6, 1024)\n",
    "        self.comb_lin2 = nn.Linear(1024, init_filters*(conv_out_size**2))\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(z_dim+6, 2)\n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, label, coord):\n",
    "        \n",
    "        #Label\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,label],dim=1)\n",
    "        \n",
    "        #Coordinate Recontruction\n",
    "        coordinate_reconstruction = self.coord_lin1(concat)\n",
    "        \n",
    "        #Image Reconstruction\n",
    "        x = self.comb_lin1(concat)\n",
    "        x = F.relu(x)\n",
    "        x = self.comb_lin2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_filters, conv_out_size, conv_out_size)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dec1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec5(x)\n",
    "        image_reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return image_reconstruction, coordinate_reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self, image, label, coord):\n",
    "        mu, log_var = self.encoder(image, label, coord)\n",
    "        \n",
    "        #print('mu: ', mu.shape)\n",
    "        #print('log_var: ', log_var.shape)\n",
    "        \n",
    "        #sample z from latent distribution q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu,std)\n",
    "        z = q.rsample()\n",
    "        #print('z shape: ', z.shape)\n",
    "        \n",
    "        img_recon, coord_recon = self.decoder(z, label, coord)\n",
    "                \n",
    "        return img_recon, coord_recon, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(mean, logscale, sample):\n",
    "    scale = torch.exp(logscale)\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "    log_pxz = dist.log_prob(sample)\n",
    "    return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "def kl_divergence(z, mu, std):\n",
    "    # --------------------------\n",
    "    # Monte carlo KL divergence\n",
    "    # --------------------------\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the equation\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # kl\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        image, label, coord = batch\n",
    "        #print(image.size())\n",
    "        #print(label)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        img_recon, coord_recon, mu, log_var, z = model(image, label, coord)\n",
    "        \n",
    "        img_recon_loss = loss(img_recon, image)\n",
    "        \n",
    "        coord_recon_loss = loss(coord_recon, coord)\n",
    "        \n",
    "#         print( str(img_recon_loss.item()) + \" \" + str(coord_recon_loss.item()) )\n",
    "        \n",
    "        recon_loss = img_recon_loss + coord_recon_loss\n",
    "        \n",
    "        std = torch.exp(log_var / 2)\n",
    "        kl = kl_divergence(z, mu, std)\n",
    "\n",
    "        elbo = (beta*kl + recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "        \n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += elbo\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset) #Investigate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_noisy = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            \n",
    "            image, label, coord = batch\n",
    "            \n",
    "            noisy_z = (torch.FloatTensor(batch_size, 3, image_size, image_size).uniform_())*256\n",
    "            noisy_z = torch.round(noisy_z)\n",
    "                \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                noisy_z = noisy_z.to(device)\n",
    "            \n",
    "            img_recon, coord_recon, mu, log_var, z = model(image, label, coord)\n",
    "            \n",
    "            #print(\"Real Vector:\" + str(z.shape))\n",
    "            #print(\"Imitation Vector:\" + str(noisy_z.shape))\n",
    "            \n",
    "            img_recon_noise, coord_recon_noise, _, _, _  = model(noisy_z, label, coord)\n",
    "            \n",
    "            if (i == int(len(val_data)/dataloader.batch_size) - 1 and ( ((epoch%val_img_out_freq))==4) ): # or epoch > 90\n",
    "                num_rows = 4\n",
    "                both = torch.cat((image.view(batch_size, 3, image_size, image_size)[:4], \n",
    "                                  img_recon.view(batch_size, 3, image_size, image_size)[:4],\n",
    "                                  img_recon_noise.view(batch_size, 3, image_size, image_size)[:4]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "            \n",
    "#             recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "            recon_loss_true = loss(img_recon, image) + loss(coord_recon, coord)\n",
    "            recon_loss_noisy = loss(img_recon_noise, image) + loss(coord_recon_noise, coord)\n",
    "        \n",
    "            std = torch.exp(log_var / 2)\n",
    "            kl = kl_divergence(z, mu, std)\n",
    "            \n",
    "#             print(\"Validation:\")\n",
    "#             print(\"kl:\" + str(kl))\n",
    "#             print(\"recon:\"+ str(recon_loss))\n",
    "\n",
    "            elbo_true = (beta*kl + recon_loss_true)\n",
    "            elbo_true = elbo_true.mean()\n",
    "        \n",
    "            elbo_noisy = (beta*kl + recon_loss_noisy)\n",
    "            elbo_noisy = elbo_noisy.mean()\n",
    "            \n",
    "            running_loss_true += elbo_true\n",
    "            \n",
    "            running_loss_noisy += elbo_noisy\n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    val_loss_true = running_loss_true/len(dataloader.dataset)\n",
    "    val_loss_noisy = running_loss_noisy/len(dataloader.dataset)\n",
    "    return val_loss_true, val_loss_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            mu, logvar = model.encoder(image.cuda(), label.cuda(), coord.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "#         print(len(latent))\n",
    "#         print(latent)\n",
    "#         print(len(target))\n",
    "#         print(target)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    drop = 0.5\n",
    "    epochs_drop = 25.0\n",
    "    lrate = initial_learning_rate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def run_each():\n",
    "    train_loss = []\n",
    "    val_loss_true = []\n",
    "    val_loss_noisy = []\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        sleep(0.2)\n",
    "        \n",
    "        learning_rate = step_decay(epoch)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        \n",
    "        val_epoch_loss_true, val_epoch_loss_noisy = validate(model, val_loader, epoch)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss_true.append(val_epoch_loss_true)\n",
    "        val_loss_noisy.append(val_epoch_loss_noisy)\n",
    "\n",
    "#         print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    return train_loss, val_loss_true, val_loss_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    train_loss, val_loss_true, val_loss_noisy = run_each()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss_true, label=\"Validation Loss (True Reconstruction)\")\n",
    "    plt.plot(range(1,epochs+1), val_loss_noisy, label=\"Validation Loss (Noisy Reconstruction)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    latent, target = generate_latent_vectors(model, val_loader)\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Latent\", \"Target\"])\n",
    "        \n",
    "        for i in range (0,len(latent)):\n",
    "            latent[i] = list(latent[i])\n",
    "            \n",
    "        wr.writerows(zip(latent, target))\n",
    "    \n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        \n",
    "        for i in range (0,len(train_loss)):\n",
    "            train_loss[i] = train_loss[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss_true)):\n",
    "            val_loss_true[i] = val_loss_true[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss_noisy)):\n",
    "            val_loss_noisy[i] = val_loss_noisy[i].item()\n",
    "            \n",
    "        wr.writerows(zip(train_loss, val_loss_true, val_loss_noisy))\n",
    "        \n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter,\n",
    "                     value,\n",
    "                     train_loss[-1],\n",
    "                     val_loss_true[-1],\n",
    "                     val_loss_noisy[-1],\n",
    "                     enc_out_dim, \n",
    "                     latent_dim, \n",
    "                     epochs,\n",
    "                     batch_size, \n",
    "                     initial_learning_rate, \n",
    "                     kernel_size, \n",
    "                     stride,\n",
    "                     padding, \n",
    "                     init_filters,\n",
    "                     dropout_pcent,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "    \"enc_out_dim\": 512,\n",
    "    \"latent_dim\": 192,\n",
    "    \"conv_out_size\": 4,\n",
    "    \n",
    "    \"epochs\" : 250,\n",
    "    \"batch_size\" : 4,\n",
    "    \"initial_learning_rate\" : 0.001,\n",
    "    \n",
    "    \"kernel_size\" : 5,\n",
    "    \"stride\" : 2,\n",
    "    \"padding\" : 2,\n",
    "    \n",
    "    \"init_filters\" : 128,\n",
    "    \n",
    "    \"dropout_pcent\": 0.0,\n",
    "    \"image_size\": 128,\n",
    "    \n",
    "    \"label_dropout_pcent\": 0.0\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [00:11<00:00, 33.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:04<00:00, 33.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:03<00:00, 46.63it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_img_out_freq = 5\n",
    "parameter_list = ['DualReconstructionProceduralImages']\n",
    "value_list = [0]\n",
    "\n",
    "for parameter in parameter_list:\n",
    "    for value in value_list:\n",
    "        #Update the values to be updated and rerun the experiment\n",
    "        enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "        latent_dim = param_dict[\"latent_dim\"]\n",
    "        conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "        epochs = param_dict[\"epochs\"]\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        initial_learning_rate = param_dict[\"initial_learning_rate\"]\n",
    "\n",
    "        kernel_size = param_dict[\"kernel_size\"]\n",
    "        stride = param_dict[\"stride\"]\n",
    "        padding = param_dict[\"padding\"]\n",
    "        init_filters = param_dict[\"init_filters\"]\n",
    "        \n",
    "        dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "        image_size = param_dict[\"image_size\"]\n",
    "        \n",
    "        label_dropout_pcent = param_dict[\"label_dropout_pcent\"]\n",
    "        \n",
    "        beta = 1\n",
    "        \n",
    "        #Move batch_size to before so its trained on the same split?\n",
    "        set_used = 'datasets/SmallProcedural'\n",
    "        train_data = ActiveVisionDataset(csv_file=set_used+'/TrainSet/rgbCSV.csv', root_dir=set_used+'/TrainSet/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "        val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        img_size = len(train_data[0][0][0])\n",
    "        model = ConditionalVAE(latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate)\n",
    "\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0])).to(device)\n",
    "        loss = nn.MSELoss(reduction = 'sum')\n",
    "        \n",
    "        \n",
    "        #Change the value to a string for later\n",
    "        value = str(value)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "        \n",
    "        #Run the Test\n",
    "        runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-88089170afd2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-88089170afd2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    This is generation stuff below!\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "This is generation stuff below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "folder = \"generation/DualReconstructionProcedural1000\"\n",
    "\n",
    "test_data = ActiveVisionDataset(csv_file= folder + '/rgbCSV.csv', root_dir=folder + '/real' , transform = torchvision.transforms.ToTensor())\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=4, shuffle=False)\n",
    "\n",
    "#Update the values to be updated and rerun the experiment\n",
    "enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "latent_dim = param_dict[\"latent_dim\"]\n",
    "conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "epochs = param_dict[\"epochs\"]\n",
    "batch_size = param_dict[\"batch_size\"]\n",
    "initial_learning_rate = param_dict[\"initial_learning_rate\"]\n",
    "\n",
    "kernel_size = param_dict[\"kernel_size\"]\n",
    "stride = param_dict[\"stride\"]\n",
    "padding = param_dict[\"padding\"]\n",
    "init_filters = param_dict[\"init_filters\"]\n",
    "\n",
    "dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "image_size = param_dict[\"image_size\"]\n",
    "\n",
    "label_dropout_pcent = param_dict[\"label_dropout_pcent\"]\n",
    "\n",
    "\n",
    "model = ConditionalVAE(latent_dim).to(device)\n",
    "noisy_image_date = (torch.FloatTensor(batch_size, 3, image_size, image_size).uniform_())*256\n",
    "model.eval()\n",
    "\n",
    "name = \"DualReconstructionProceduralImages0\"\n",
    "PATH = os.path.join(os.getcwd(), \"outputs\", name, name+\".pth\")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "def generate_images(model, dataloader):\n",
    "\n",
    "    true_images = []\n",
    "    generated_images = []\n",
    "    coordinate = []\n",
    "    SSIM = []\n",
    "    MSE = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            \n",
    "            noisy_z = (torch.FloatTensor(batch_size, 3, image_size, image_size).uniform_())*256\n",
    "            noisy_z = torch.round(noisy_z)\n",
    "                \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                noisy_z = noisy_z.to(device)\n",
    "            \n",
    "            #img_recon, coord_recon, mu, log_var, z = model(image, label, coord)\n",
    "            #print(\"Real Vector:\" + str(z.shape))\n",
    "            #print(\"Imitation Vector:\" + str(noisy_z.shape))\n",
    "            \n",
    "            img_noise, _, _, _, _  = model(noisy_z, label, coord)\n",
    "            \n",
    "            for img in img_noise:\n",
    "                save_image(img.cpu(), f\"{folder}/generated/generated{count}.png\")\n",
    "                count += 1    \n",
    "            true_images.extend(image.cpu().detach().numpy())\n",
    "            generated_images.extend(img_noise.cpu().detach().numpy())\n",
    "        \n",
    "#         print(true_images[0].size())\n",
    "        for i in range(len(true_images)):\n",
    "            true_image = np.moveaxis(true_images[i], 0, 2)  \n",
    "            generated_image = np.moveaxis(generated_images[i], 0, 2)  \n",
    "            ssim_noise = ssim(true_image, generated_image, multichannel=True,data_range=255)\n",
    "            SSIM.append(ssim_noise)\n",
    "            mse_loss = loss(torch.tensor(generated_image), torch.tensor(true_image))\n",
    "            MSE.append(mse_loss)\n",
    "            \n",
    "        with open(f'{folder}/SSIM.csv','w', newline='') as f:\n",
    "            wr = csv.writer(f)\n",
    "            wr.writerow([\"SSIM\", \"MSE\"])\n",
    "\n",
    "            for i in range (0,len(SSIM)):\n",
    "                SSIM[i] = SSIM[i].item()\n",
    "                MSE[i] = MSE[i].item()\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            wr.writerows(zip(SSIM, MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [01:54<00:00, 13.15it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_images(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:03<00:00, 48.84it/s]\n"
     ]
    }
   ],
   "source": [
    "parameter = 'DualReconstructionProceduralImages'\n",
    "value = '0'\n",
    "\n",
    "val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#Update the values to be updated and rerun the experiment\n",
    "enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "latent_dim = param_dict[\"latent_dim\"]\n",
    "conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "epochs = param_dict[\"epochs\"]\n",
    "batch_size = param_dict[\"batch_size\"]\n",
    "initial_learning_rate = param_dict[\"initial_learning_rate\"]\n",
    "\n",
    "kernel_size = param_dict[\"kernel_size\"]\n",
    "stride = param_dict[\"stride\"]\n",
    "padding = param_dict[\"padding\"]\n",
    "init_filters = param_dict[\"init_filters\"]\n",
    "\n",
    "dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "image_size = param_dict[\"image_size\"]\n",
    "\n",
    "model = ConditionalVAE(latent_dim).to(device)\n",
    "noisy_image_date = (torch.FloatTensor(batch_size, 3, image_size, image_size).uniform_())*256\n",
    "model.eval()\n",
    "\n",
    "latent, target = generate_latent_vectors(model, val_loader)\n",
    "\n",
    "with open('outputs/'+parameter+value+'/ordered_sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"Latent\", \"Target\"])\n",
    "\n",
    "    for i in range (0,len(latent)):\n",
    "        latent[i] = list(latent[i])\n",
    "\n",
    "    wr.writerows(zip(latent, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE\n",
    "\n",
    "With Input:\n",
    "- Image Label\n",
    "- Coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        #image = image/(image.max()/255.0)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #print(shape_label)\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        #print(cam_loc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        self.img_lin1 = nn.Linear(init_filters*(conv_out_size**2), 1024)\n",
    "        self.img_lin2 = nn.Linear(4096, 1024)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(1024+3+6,256)\n",
    "        self.comb_lin2 = nn.Linear(512,256)\n",
    "\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.sigma = nn.Linear(256, z_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(1024+3+6)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, image, label, coord):\n",
    "        \n",
    "        #Image                                                                 #print(\"before anything\") #print(image.shape)\n",
    "        x = self.conv1(image)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\" + str(x.shape))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"after flatten:\" + str(x.shape))\n",
    "\n",
    "        x = self.img_lin1(x)\n",
    "        x = F.relu(x)\n",
    "#         x = self.img_lin2(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        #Label                                                  #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        y = label\n",
    "#         y = self.label_lin1(label)\n",
    "#         y = F.relu(y)\n",
    "        \n",
    "        #Coordinate\n",
    "        z = coord\n",
    "#         z = self.coord_lin1(coord)\n",
    "#         z = F.relu(z)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y,z],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "                                                                               #print(label) print(label.shape) print(coord) \n",
    "                                                                 #print(coord.shape) print(y) print(y.shape) #print(x.shape)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "        #print(\"after combination:\" + str(x.shape))\n",
    "#         x = self.bn1(concat)\n",
    "        x = self.dropout(concat)\n",
    "        x = self.comb_lin1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        \n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.img_lin1 = nn.Linear(z_dim, 256)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(256+3+6, 1024)\n",
    "        self.comb_lin2 = nn.Linear(1024, init_filters*(conv_out_size**2))\n",
    "        \n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "    def forward(self, z, label, coord):\n",
    "        \n",
    "        #Latent Vector\n",
    "        x = self.img_lin1(z)\n",
    "        \n",
    "        #Label\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        y = label\n",
    "#         y = self.label_lin1(label)\n",
    "#         y = F.relu(y)\n",
    "        \n",
    "        #Coordinate\n",
    "        z = coord        \n",
    "#         z = self.coord_lin1(coord)\n",
    "#         z = F.relu(z)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y,z],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "        x = self.dropout(concat)\n",
    "        x = self.comb_lin1(x)\n",
    "        x=F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.comb_lin2(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_filters, conv_out_size, conv_out_size)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dec1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self, image, label, coord):\n",
    "        mu, log_var = self.encoder(image, label, coord)\n",
    "        \n",
    "        #print('mu: ', mu.shape)\n",
    "        #print('log_var: ', log_var.shape)\n",
    "        \n",
    "        #sample z from latent distribution q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu,std)\n",
    "        z = q.rsample()\n",
    "        #print('z shape: ', z.shape)\n",
    "        \n",
    "        reconstruction = self.decoder(z, label, coord)\n",
    "                \n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(mean, logscale, sample):\n",
    "    scale = torch.exp(logscale)\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "    log_pxz = dist.log_prob(sample)\n",
    "    return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "def kl_divergence(z, mu, std):\n",
    "    # --------------------------\n",
    "    # Monte carlo KL divergence\n",
    "    # --------------------------\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the equation\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # kl\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        image, label, coord = batch\n",
    "        #print(image.size())\n",
    "        #print(label)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "        \n",
    "        #print(reconstruction.shape)\n",
    "        \n",
    "        #image = image.to(torch.device('cpu'))\n",
    "#         recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "        recon_loss = loss(reconstruction, image)\n",
    "        \n",
    "        std = torch.exp(log_var / 2)\n",
    "        kl = kl_divergence(z, mu, std)\n",
    "        \n",
    "#         print(\"Training:\")\n",
    "#         print(\"kl:\" + str(kl))\n",
    "#         print(\"recon:\"+ str(recon_loss))\n",
    "\n",
    "        elbo = (0.6*kl + recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "        \n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += elbo\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset) #Investigate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            \n",
    "            image, label, coord = batch\n",
    "            \n",
    "            random_z = torch.empty(batch_size, latent_dim).normal_(mean=0,std=1.0)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                random_z = random_z.to(device)\n",
    "            \n",
    "            _, mu, log_var, z = model(image, label, coord)\n",
    "            \n",
    "            #print(\"Real Vector:\" + str(z.shape))\n",
    "            #print(\"Imitation Vector:\" + str(random_z.shape))\n",
    "            \n",
    "            reconstruction = model.decoder(random_z, label, coord)\n",
    "            \n",
    "            if (i == int(len(val_data)/dataloader.batch_size) - 1 and ( ((epoch%val_img_out_freq))==4) ): # or epoch > 90\n",
    "                num_rows = 4\n",
    "                both = torch.cat((image.view(batch_size, 3, image_size, image_size)[:4], \n",
    "                                  reconstruction.view(batch_size, 3, image_size, image_size)[:4]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "            \n",
    "#             recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "            recon_loss = loss(reconstruction, image)\n",
    "            \n",
    "            std = torch.exp(log_var / 2)\n",
    "            kl = kl_divergence(z, mu, std)\n",
    "            \n",
    "#             print(\"Validation:\")\n",
    "#             print(\"kl:\" + str(kl))\n",
    "#             print(\"recon:\"+ str(recon_loss))\n",
    "\n",
    "            elbo = (0.6*kl + recon_loss)\n",
    "            elbo = elbo.mean()\n",
    "            \n",
    "            running_loss += elbo \n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            mu, logvar = model.encoder(image.cuda(), label.cuda(), coord.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "#         print(len(latent))\n",
    "#         print(latent)\n",
    "#         print(len(target))\n",
    "#         print(target)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def run_each():\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        sleep(0.2)\n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        val_epoch_loss = validate(model, val_loader, epoch)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "\n",
    "#         print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    train_loss, val_loss = run_each()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    latent, target = generate_latent_vectors(model, val_loader)\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Latent\", \"Target\"])\n",
    "        \n",
    "        for i in range (0,len(latent)):\n",
    "            latent[i] = list(latent[i])\n",
    "            \n",
    "        wr.writerows(zip(latent, target))\n",
    "    \n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        \n",
    "        for i in range (0,len(train_loss)):\n",
    "            train_loss[i] = train_loss[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss)):\n",
    "            val_loss[i] = val_loss[i].item()\n",
    "            \n",
    "        wr.writerows(zip(train_loss, val_loss))\n",
    "        \n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter, value ,train_loss[-1], val_loss[-1], \n",
    "                     enc_out_dim, \n",
    "                     latent_dim, \n",
    "                     epochs,\n",
    "                     batch_size, \n",
    "                     learning_rate, \n",
    "                     kernel_size, \n",
    "                     stride,\n",
    "                     padding, \n",
    "                     init_filters,\n",
    "                     dropout_pcent,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "    \"enc_out_dim\": 512,\n",
    "    \"latent_dim\": 128,\n",
    "    \"conv_out_size\": 4,\n",
    "    \n",
    "    \"epochs\" : 100,\n",
    "    \"batch_size\" : 8,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \n",
    "    \"kernel_size\" : 5,\n",
    "    \"stride\" : 2,\n",
    "    \"padding\" : 2,\n",
    "    \n",
    "    \"init_filters\" : 128,\n",
    "    \n",
    "    \"dropout_pcent\": 0.4,\n",
    "    \"image_size\": 128,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 188/188 [00:05<00:00, 33.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:01<00:00, 52.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 85.79it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_img_out_freq = 5\n",
    "parameter_list = ['MSE_Long_Run_']\n",
    "value_list = [0]\n",
    "\n",
    "for parameter in parameter_list:\n",
    "    for value in value_list:\n",
    "        #Update the values to be updated and rerun the experiment\n",
    "        enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "        latent_dim = param_dict[\"latent_dim\"]\n",
    "        conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "        epochs = param_dict[\"epochs\"]\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        learning_rate = param_dict[\"learning_rate\"]\n",
    "\n",
    "        kernel_size = param_dict[\"kernel_size\"]\n",
    "        stride = param_dict[\"stride\"]\n",
    "        padding = param_dict[\"padding\"]\n",
    "        init_filters = param_dict[\"init_filters\"]\n",
    "        \n",
    "        dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "        image_size = param_dict[\"image_size\"]\n",
    "        \n",
    "        #Move batch_size to before so its trained on the same split?\n",
    "        set_used = 'datasets/SmallGrey'\n",
    "        train_data = ActiveVisionDataset(csv_file=set_used+'/TrainSet/rgbCSV.csv', root_dir=set_used+'/TrainSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        img_size = len(train_data[0][0][0])\n",
    "        model = ConditionalVAE(latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0])).to(device)\n",
    "        loss = nn.MSELoss(reduction = 'sum')\n",
    "        \n",
    "        \n",
    "        #Change the value to a string for later\n",
    "        value = str(value)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "        \n",
    "        #Run the Test\n",
    "        runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

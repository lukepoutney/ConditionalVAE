{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE\n",
    "\n",
    "With Input:\n",
    "- Image Label\n",
    "- Coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        #image = image/(image.max()/255.0)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #print(shape_label)\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        #print(cam_loc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_kernel, kernel_size=kernel_size, stride=stride_enc, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, stride=stride_enc, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, stride=stride_enc, padding=padding\n",
    "        )\n",
    "\n",
    "        self.img_lin1 = nn.Linear(init_kernel*(conv_out_size**2), 1024-9)\n",
    "\n",
    "        self.lin1 = nn.Linear(1024,512)\n",
    "        self.lin2 = nn.Linear(512,256)\n",
    "\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.sigma = nn.Linear(256, z_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "    def forward(self, image, label, coord):\n",
    "        \n",
    "        #Image                                                                 #print(\"before anything\") #print(image.shape)\n",
    "        x = self.conv1(image)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\" + str(x.shape))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"after flatten:\" + str(x.shape))\n",
    "\n",
    "        x = self.img_lin1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #Label and Coordinate                                                  #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        y = torch.cat([coord,label],dim=1)\n",
    "        x = torch.cat([x,y],dim=1)\n",
    "                                                                               #print(label) print(label.shape) print(coord) \n",
    "                                                                 #print(coord.shape) print(y) print(y.shape) #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        \n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.img_lin1 = nn.Linear(z_dim, 256-9)\n",
    "        \n",
    "        self.lin1 = nn.Linear(256,512)\n",
    "        self.lin2 = nn.Linear(512,1024)\n",
    "        self.lin3 = nn.Linear(1024, init_kernel*(conv_out_size**2))\n",
    "        \n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, stride=stride_dec, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, stride=stride_dec, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*4, out_channels=3, kernel_size=kernel_size, stride=stride_dec, padding=padding\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "    def forward(self, z, label, coord):\n",
    "        \n",
    "        x = self.img_lin1(z)\n",
    "        \n",
    "        #Label and Coordinate\n",
    "        #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        y = torch.cat([coord,label],dim=1)\n",
    "        x = torch.cat([x,y],dim=1)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin1(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin3(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_kernel, conv_out_size, conv_out_size)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dec1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec2(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec3(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self, image, label, coord):\n",
    "        mu, log_var = self.encoder(image, label, coord)\n",
    "        \n",
    "        #print('mu: ', mu.shape)\n",
    "        #print('log_var: ', log_var.shape)\n",
    "        \n",
    "        #sample z from latent distribution q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu,std)\n",
    "        z = q.rsample()\n",
    "        #print('z shape: ', z.shape)\n",
    "        \n",
    "        reconstruction = self.decoder(z, label, coord)\n",
    "                \n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(mean, logscale, sample):\n",
    "    scale = torch.exp(logscale)\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "    log_pxz = dist.log_prob(sample)\n",
    "    return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "def kl_divergence(z, mu, std):\n",
    "    # --------------------------\n",
    "    # Monte carlo KL divergence\n",
    "    # --------------------------\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the equation\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # kl\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True) #???\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        image, label, coord = batch\n",
    "        #print(image.size())\n",
    "        #print(label)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "        \n",
    "        #print(reconstruction.shape)\n",
    "        \n",
    "        #image = image.to(torch.device('cpu'))\n",
    "        recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "        \n",
    "        std = torch.exp(log_var / 2)\n",
    "        kl = kl_divergence(z, mu, std)\n",
    "\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "        \n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += elbo\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset) #Investigate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            \n",
    "            image, label, coord = batch\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                \n",
    "            reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "            \n",
    "            if (i == int(len(val_data)/dataloader.batch_size) - 1 and ( ((epoch%val_img_out_freq))==0) ): # or epoch > 90\n",
    "                num_rows = 4\n",
    "                both = torch.cat((image.view(batch_size, 3, 128, 128)[:8], \n",
    "                                  reconstruction.view(batch_size, 3, 128, 128)[:8]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "            \n",
    "            recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "            \n",
    "            std = torch.exp(log_var / 2)\n",
    "            kl = kl_divergence(z, mu, std)\n",
    "\n",
    "            elbo = (kl - recon_loss)\n",
    "            elbo = elbo.mean()\n",
    "            \n",
    "            running_loss += elbo \n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            mu, logvar = model.encoder(image.cuda(), label.cuda(), coord.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "#         print(len(latent))\n",
    "#         print(latent)\n",
    "#         print(len(target))\n",
    "#         print(target)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def run_each():\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        sleep(0.2)\n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        val_epoch_loss = validate(model, val_loader, epoch)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "\n",
    "#         print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    train_loss, val_loss = run_each()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    latent, target = generate_latent_vectors(model, val_loader)\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Latent\", \"Target\"])\n",
    "        \n",
    "        for i in range (0,len(latent)):\n",
    "            latent[i] = list(latent[i])\n",
    "            \n",
    "        wr.writerows(zip(latent, target))\n",
    "    \n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        \n",
    "        for i in range (0,len(train_loss)):\n",
    "            train_loss[i] = train_loss[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss)):\n",
    "            val_loss[i] = val_loss[i].item()\n",
    "            \n",
    "        wr.writerows(zip(train_loss, val_loss))\n",
    "        \n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter, value ,train_loss[-1], val_loss[-1], \n",
    "                     enc_out_dim, \n",
    "                     latent_dim, \n",
    "                     epochs,\n",
    "                     batch_size, \n",
    "                     learning_rate, \n",
    "                     kernel_size, \n",
    "                     stride_enc,\n",
    "                     stride_dec,\n",
    "                     padding, \n",
    "                     init_kernel,\n",
    "                     dropout_pcent,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "    \"enc_out_dim\": 512,\n",
    "    \"latent_dim\": 128,\n",
    "    \"conv_out_size\": 64,\n",
    "    \n",
    "    \"epochs\" : 100,\n",
    "    \"batch_size\" : 4,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \n",
    "    \"kernel_size\" : 3,\n",
    "    \"stride_encoding\" : 2,\n",
    "    \"stride_decoding\" : 2, ###########################################################\n",
    "    \"padding\" : 1,\n",
    "    \n",
    "    \"init_kernel\" : 8,\n",
    "    \n",
    "    \"dropout_pcent\": 0.4\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                 | 1/375 [00:01<08:58,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                | 9/375 [00:01<04:23,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███                                                                              | 14/375 [00:01<03:04,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▊                                                                            | 22/375 [00:02<01:33,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▉                                                                          | 32/375 [00:02<00:48,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▊                                                                         | 36/375 [00:02<00:36,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▋                                                                       | 45/375 [00:02<00:21, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▉                                                                     | 55/375 [00:02<00:14, 22.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▉                                                                    | 60/375 [00:02<00:12, 26.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████                                                                  | 70/375 [00:03<00:09, 32.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▏                                                                | 75/375 [00:03<00:08, 35.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▎                                                              | 85/375 [00:03<00:07, 38.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▍                                                             | 90/375 [00:03<00:11, 24.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "before flatten:torch.Size([4, 8, 64, 64])\n",
      "after flatten:torch.Size([4, 32768])\n",
      "torch.Size([4, 3, 128, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d7a3a2560fb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m#Run the Test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mrunall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-94278bdd81a8>\u001b[0m in \u001b[0;36mrunall\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrunall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_each\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Train Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Validation Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6f5abe4a2b75>\u001b[0m in \u001b[0;36mrun_each\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch+1} of {epochs}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtrain_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mval_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-24e6e8e0ccd4>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0melbo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melbo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0melbo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_img_out_freq = 5\n",
    "parameter_list = ['SmallSetOddKernel']\n",
    "value_list = [0]\n",
    "\n",
    "for parameter in parameter_list:\n",
    "    for value in value_list:\n",
    "        #Update the values to be updated and rerun the experiment\n",
    "        enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "        latent_dim = param_dict[\"latent_dim\"]\n",
    "        conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "        epochs = param_dict[\"epochs\"]\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        learning_rate = param_dict[\"learning_rate\"]\n",
    "\n",
    "        kernel_size = param_dict[\"kernel_size\"]\n",
    "        stride_enc = param_dict[\"stride_encoding\"]\n",
    "        stride_dec = param_dict[\"stride_decoding\"]\n",
    "        padding = param_dict[\"padding\"]\n",
    "        init_kernel = param_dict[\"init_kernel\"]\n",
    "        \n",
    "        dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "        \n",
    "        #Move batch_size to before so its trained on the same split?\n",
    "        set_used = 'datasets/SmallSet'\n",
    "        train_data = ActiveVisionDataset(csv_file=set_used+'/TrainSet/rgbCSV.csv', root_dir=set_used+'/TrainSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        img_size = len(train_data[0][0][0])\n",
    "        model = ConditionalVAE(latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0])).to(device)\n",
    "        \n",
    "        \n",
    "        #Change the value to a string for later\n",
    "        value = str(value)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "        \n",
    "        #Run the Test\n",
    "        runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

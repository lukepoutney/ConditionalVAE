{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE\n",
    "\n",
    "With Input:\n",
    "- Image Label\n",
    "- Coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        #image = image/(image.max()/255.0)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #print(shape_label)\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        #print(cam_loc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        self.img_lin1 = nn.Linear(init_filters*(conv_out_size**2), 1024)\n",
    "        self.img_lin2 = nn.Linear(4096, 1024)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(1024+3+6,256)\n",
    "        self.comb_lin2 = nn.Linear(512,256)\n",
    "\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.sigma = nn.Linear(256, z_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(1024+3+6)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, image, label, coord):\n",
    "        \n",
    "        #Image                                                                 #print(\"before anything\") #print(image.shape)\n",
    "        x = self.conv1(image)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\" + str(x.shape))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"after flatten:\" + str(x.shape))\n",
    "\n",
    "        x = self.img_lin1(x)\n",
    "        x = F.relu(x)\n",
    "#         x = self.img_lin2(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        #Label                                                  #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        y = label\n",
    "#         y = self.label_lin1(label)\n",
    "#         y = F.relu(y)\n",
    "        \n",
    "        #Coordinate\n",
    "        z = coord\n",
    "#         z = self.coord_lin1(coord)\n",
    "#         z = F.relu(z)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y,z],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "                                                                               #print(label) print(label.shape) print(coord) \n",
    "                                                                 #print(coord.shape) print(y) print(y.shape) #print(x.shape)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "        #print(\"after combination:\" + str(x.shape))\n",
    "#         x = self.bn1(concat)\n",
    "#         x = self.dropout(concat)\n",
    "        x = self.comb_lin1(concat)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        \n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.img_lin1 = nn.Linear(z_dim, 256)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(256+3+6, 1024)\n",
    "        self.comb_lin2 = nn.Linear(1024, init_filters*(conv_out_size**2))\n",
    "        \n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "    def forward(self, z, label, coord):\n",
    "        \n",
    "        #Latent Vector\n",
    "        x = self.img_lin1(z)\n",
    "        \n",
    "        #Label\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        y = label\n",
    "#         y = self.label_lin1(label)\n",
    "#         y = F.relu(y)\n",
    "        \n",
    "        #Coordinate\n",
    "        z = coord        \n",
    "#         z = self.coord_lin1(coord)\n",
    "#         z = F.relu(z)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y,z],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "#         x = self.dropout(concat)\n",
    "        x = self.comb_lin1(concat)\n",
    "        x=F.relu(x)\n",
    "        x = self.comb_lin2(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_filters, conv_out_size, conv_out_size)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dec1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self, image, label, coord):\n",
    "        mu, log_var = self.encoder(image, label, coord)\n",
    "        \n",
    "        #print('mu: ', mu.shape)\n",
    "        #print('log_var: ', log_var.shape)\n",
    "        \n",
    "        #sample z from latent distribution q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu,std)\n",
    "        z = q.rsample()\n",
    "        #print('z shape: ', z.shape)\n",
    "        \n",
    "        reconstruction = self.decoder(z, label, coord)\n",
    "                \n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(mean, logscale, sample):\n",
    "    scale = torch.exp(logscale)\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "    log_pxz = dist.log_prob(sample)\n",
    "    return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "def kl_divergence(z, mu, std):\n",
    "    # --------------------------\n",
    "    # Monte carlo KL divergence\n",
    "    # --------------------------\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the equation\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # kl\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        image, label, coord = batch\n",
    "        #print(image.size())\n",
    "        #print(label)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "        \n",
    "        #print(reconstruction.shape)\n",
    "        \n",
    "        #image = image.to(torch.device('cpu'))\n",
    "        recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "        \n",
    "        std = torch.exp(log_var / 2)\n",
    "        kl = kl_divergence(z, mu, std)\n",
    "\n",
    "        elbo = ( kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "        \n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += elbo\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset) #Investigate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            \n",
    "            image, label, coord = batch\n",
    "            \n",
    "            random_z = torch.empty(batch_size, latent_dim).normal_(mean=0,std=1.0)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                random_z = random_z.to(device)\n",
    "            \n",
    "            _, mu, log_var, z = model(image, label, coord)\n",
    "            \n",
    "            #print(\"Real Vector:\" + str(z.shape))\n",
    "            #print(\"Imitation Vector:\" + str(random_z.shape))\n",
    "            \n",
    "            reconstruction = model.decoder(random_z, label, coord)\n",
    "            \n",
    "            if (i == int(len(val_data)/dataloader.batch_size) - 1 and ( ((epoch%val_img_out_freq))==4) ): # or epoch > 90\n",
    "                num_rows = 4\n",
    "                both = torch.cat((image.view(batch_size, 3, image_size, image_size)[:4], \n",
    "                                  reconstruction.view(batch_size, 3, image_size, image_size)[:4]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "            \n",
    "            recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "            \n",
    "            std = torch.exp(log_var / 2)\n",
    "            kl = kl_divergence(z, mu, std)\n",
    "\n",
    "            elbo = (kl - recon_loss)\n",
    "            elbo = elbo.mean()\n",
    "            \n",
    "            running_loss += elbo \n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            mu, logvar = model.encoder(image.cuda(), label.cuda(), coord.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "#         print(len(latent))\n",
    "#         print(latent)\n",
    "#         print(len(target))\n",
    "#         print(target)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def run_each():\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        sleep(0.2)\n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        val_epoch_loss = validate(model, val_loader, epoch)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "\n",
    "#         print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    train_loss, val_loss = run_each()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    latent, target = generate_latent_vectors(model, val_loader)\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Latent\", \"Target\"])\n",
    "        \n",
    "        for i in range (0,len(latent)):\n",
    "            latent[i] = list(latent[i])\n",
    "            \n",
    "        wr.writerows(zip(latent, target))\n",
    "    \n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        \n",
    "        for i in range (0,len(train_loss)):\n",
    "            train_loss[i] = train_loss[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss)):\n",
    "            val_loss[i] = val_loss[i].item()\n",
    "            \n",
    "        wr.writerows(zip(train_loss, val_loss))\n",
    "        \n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter, value ,train_loss[-1], val_loss[-1], \n",
    "                     enc_out_dim, \n",
    "                     latent_dim, \n",
    "                     epochs,\n",
    "                     batch_size, \n",
    "                     learning_rate, \n",
    "                     kernel_size, \n",
    "                     stride,\n",
    "                     padding, \n",
    "                     init_filters,\n",
    "                     dropout_pcent,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "    \"enc_out_dim\": 512,\n",
    "    \"latent_dim\": 128,\n",
    "    \"conv_out_size\": 4,\n",
    "    \n",
    "    \"epochs\" : 25,\n",
    "    \"batch_size\" : 8,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \n",
    "    \"kernel_size\" : 5,\n",
    "    \"stride\" : 2,\n",
    "    \"padding\" : 2,\n",
    "    \n",
    "    \"init_filters\" : 128,\n",
    "    \n",
    "    \"dropout_pcent\": 0.0,\n",
    "    \"image_size\": 128,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 188/188 [00:06<00:00, 28.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:01<00:00, 64.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 104.66it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_img_out_freq = 5\n",
    "parameter_list = ['Random_Test_']\n",
    "value_list = [0]\n",
    "\n",
    "for parameter in parameter_list:\n",
    "    for value in value_list:\n",
    "        #Update the values to be updated and rerun the experiment\n",
    "        enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "        latent_dim = param_dict[\"latent_dim\"]\n",
    "        conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "        epochs = param_dict[\"epochs\"]\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        learning_rate = param_dict[\"learning_rate\"]\n",
    "\n",
    "        kernel_size = param_dict[\"kernel_size\"]\n",
    "        stride = param_dict[\"stride\"]\n",
    "        padding = param_dict[\"padding\"]\n",
    "        init_filters = param_dict[\"init_filters\"]\n",
    "        \n",
    "        dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "        image_size = param_dict[\"image_size\"]\n",
    "        \n",
    "        #Move batch_size to before so its trained on the same split?\n",
    "        set_used = 'datasets/SmallGrey'\n",
    "        train_data = ActiveVisionDataset(csv_file=set_used+'/TrainSet/rgbCSV.csv', root_dir=set_used+'/TrainSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        img_size = len(train_data[0][0][0])\n",
    "        model = ConditionalVAE(latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0])).to(device)\n",
    "        \n",
    "        \n",
    "        #Change the value to a string for later\n",
    "        value = str(value)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "        \n",
    "        #Run the Test\n",
    "        runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

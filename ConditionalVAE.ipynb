{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE\n",
    "\n",
    "With Input:\n",
    "- Image Label\n",
    "- Coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        #image = image/(image.max()/255.0)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #print(shape_label)\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        #print(cam_loc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        self.img_lin1 = nn.Linear(init_filters*(conv_out_size**2), 1024)\n",
    "        self.img_lin2 = nn.Linear(4096, 1024)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(1024+3+6,256)\n",
    "        self.comb_lin2 = nn.Linear(512,256)\n",
    "\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.sigma = nn.Linear(256, z_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(1024+3+6)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, image, label, coord):\n",
    "        \n",
    "        #Image                                                                 #print(\"before anything\") #print(image.shape)\n",
    "        x = self.conv1(image)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "#         x = self.conv4(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv5(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\" + str(x.shape))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"after flatten:\" + str(x.shape))\n",
    "\n",
    "        x = self.img_lin1(x)\n",
    "        x = F.relu(x)\n",
    "#         x = self.img_lin2(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        #Label                                                  #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        y = label\n",
    "#         y = self.label_lin1(label)\n",
    "#         y = F.relu(y)\n",
    "        \n",
    "        #Coordinate\n",
    "        z = coord\n",
    "#         z = self.coord_lin1(coord)\n",
    "#         z = F.relu(z)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y,z],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "                                                                               #print(label) print(label.shape) print(coord) \n",
    "                                                                 #print(coord.shape) print(y) print(y.shape) #print(x.shape)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "        #print(\"after combination:\" + str(x.shape))\n",
    "#         x = self.bn1(concat)\n",
    "        x = self.comb_lin1(concat)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        \n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.img_lin1 = nn.Linear(z_dim, 256)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(256+3+6, 1024)\n",
    "        self.comb_lin2 = nn.Linear(1024, init_filters*(conv_out_size**2))\n",
    "        \n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "    def forward(self, z, label, coord):\n",
    "        \n",
    "        #Latent Vector\n",
    "        x = self.img_lin1(z)\n",
    "        \n",
    "        #Label\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        y = label\n",
    "#         y = self.label_lin1(label)\n",
    "#         y = F.relu(y)\n",
    "        \n",
    "        #Coordinate\n",
    "        z = coord        \n",
    "#         z = self.coord_lin1(coord)\n",
    "#         z = F.relu(z)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y,z],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "        x = self.comb_lin1(concat)\n",
    "        x=F.relu(x)\n",
    "        x = self.comb_lin2(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_filters, conv_out_size, conv_out_size)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dec1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dec4(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self, image, label, coord):\n",
    "        mu, log_var = self.encoder(image, label, coord)\n",
    "        \n",
    "        #print('mu: ', mu.shape)\n",
    "        #print('log_var: ', log_var.shape)\n",
    "        \n",
    "        #sample z from latent distribution q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu,std)\n",
    "        z = q.rsample()\n",
    "        #print('z shape: ', z.shape)\n",
    "        \n",
    "        reconstruction = self.decoder(z, label, coord)\n",
    "                \n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(mean, logscale, sample):\n",
    "    scale = torch.exp(logscale)\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "    log_pxz = dist.log_prob(sample)\n",
    "    return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "def kl_divergence(z, mu, std):\n",
    "    # --------------------------\n",
    "    # Monte carlo KL divergence\n",
    "    # --------------------------\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the equation\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # kl\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        image, label, coord = batch\n",
    "        #print(image.size())\n",
    "        #print(label)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "        \n",
    "        #print(reconstruction.shape)\n",
    "        \n",
    "        #image = image.to(torch.device('cpu'))\n",
    "        recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "        \n",
    "        std = torch.exp(log_var / 2)\n",
    "        kl = kl_divergence(z, mu, std)\n",
    "\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "        \n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += elbo\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset) #Investigate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            \n",
    "            image, label, coord = batch\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                \n",
    "            reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "            \n",
    "            if (i == int(len(val_data)/dataloader.batch_size) - 1 and ( ((epoch%val_img_out_freq))==4) ): # or epoch > 90\n",
    "                num_rows = 4\n",
    "                both = torch.cat((image.view(batch_size, 3, image_size, image_size)[:4], \n",
    "                                  reconstruction.view(batch_size, 3, image_size, image_size)[:4]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "            \n",
    "            recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "            \n",
    "            std = torch.exp(log_var / 2)\n",
    "            kl = kl_divergence(z, mu, std)\n",
    "\n",
    "            elbo = (kl - recon_loss)\n",
    "            elbo = elbo.mean()\n",
    "            \n",
    "            running_loss += elbo \n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            mu, logvar = model.encoder(image.cuda(), label.cuda(), coord.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "#         print(len(latent))\n",
    "#         print(latent)\n",
    "#         print(len(target))\n",
    "#         print(target)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def run_each():\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        sleep(0.2)\n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        val_epoch_loss = validate(model, val_loader, epoch)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "\n",
    "#         print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    train_loss, val_loss = run_each()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    latent, target = generate_latent_vectors(model, val_loader)\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Latent\", \"Target\"])\n",
    "        \n",
    "        for i in range (0,len(latent)):\n",
    "            latent[i] = list(latent[i])\n",
    "            \n",
    "        wr.writerows(zip(latent, target))\n",
    "    \n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        \n",
    "        for i in range (0,len(train_loss)):\n",
    "            train_loss[i] = train_loss[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss)):\n",
    "            val_loss[i] = val_loss[i].item()\n",
    "            \n",
    "        wr.writerows(zip(train_loss, val_loss))\n",
    "        \n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter, value ,train_loss[-1], val_loss[-1], \n",
    "                     enc_out_dim, \n",
    "                     latent_dim, \n",
    "                     epochs,\n",
    "                     batch_size, \n",
    "                     learning_rate, \n",
    "                     kernel_size, \n",
    "                     stride,\n",
    "                     padding, \n",
    "                     init_filters,\n",
    "                     dropout_pcent,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "    \"enc_out_dim\": 512,\n",
    "    \"latent_dim\": 128,\n",
    "    \"conv_out_size\": 32,\n",
    "    \n",
    "    \"epochs\" : 25,\n",
    "    \"batch_size\" : 8,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \n",
    "    \"kernel_size\" : 5,\n",
    "    \"stride\" : 2,\n",
    "    \"padding\" : 2,\n",
    "    \n",
    "    \"init_filters\" : 128,\n",
    "    \n",
    "    \"dropout_pcent\": 0.0,\n",
    "    \"image_size\": 256,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 188/188 [00:24<00:00,  7.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:01<00:00, 42.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:01<00:00, 51.80it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_img_out_freq = 5\n",
    "parameter_list = ['256Image']\n",
    "value_list = [2]\n",
    "\n",
    "for parameter in parameter_list:\n",
    "    for value in value_list:\n",
    "        #Update the values to be updated and rerun the experiment\n",
    "        enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "        latent_dim = param_dict[\"latent_dim\"]\n",
    "        conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "        epochs = param_dict[\"epochs\"]\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        learning_rate = param_dict[\"learning_rate\"]\n",
    "\n",
    "        kernel_size = param_dict[\"kernel_size\"]\n",
    "        stride = param_dict[\"stride\"]\n",
    "        padding = param_dict[\"padding\"]\n",
    "        init_filters = param_dict[\"init_filters\"]\n",
    "        \n",
    "        dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "        image_size = param_dict[\"image_size\"]\n",
    "        \n",
    "        #Move batch_size to before so its trained on the same split?\n",
    "        set_used = 'datasets/256SmallGrey'\n",
    "        train_data = ActiveVisionDataset(csv_file=set_used+'/TrainSet/rgbCSV.csv', root_dir=set_used+'/TrainSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        img_size = len(train_data[0][0][0])\n",
    "        model = ConditionalVAE(latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0])).to(device)\n",
    "        \n",
    "        \n",
    "        #Change the value to a string for later\n",
    "        value = str(value)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "        \n",
    "        #Run the Test\n",
    "        runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

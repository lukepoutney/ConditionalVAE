{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE\n",
    "\n",
    "With Input:\n",
    "- Image Label\n",
    "- Coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        #image = image/(image.max()/255.0)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #print(shape_label)\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        #print(cam_loc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_kernel, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, stride=2, padding=padding\n",
    "        )\n",
    "\n",
    "        self.img_lin1 = nn.Linear(init_kernel*(conv_out_size**2), 2048)\n",
    "        self.img_lin2 = nn.Linear(2048, 512)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,64)\n",
    "        self.label_lin2 = nn.Linear(64,256)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,64)\n",
    "        self.coord_lin2 = nn.Linear(64,256)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(1024,512)\n",
    "        self.comb_lin2 = nn.Linear(512,256)\n",
    "\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.sigma = nn.Linear(256, z_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "    def forward(self, image, label, coord):\n",
    "        \n",
    "        #Image                                                                 #print(\"before anything\") #print(image.shape)\n",
    "        x = self.conv1(image)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\" + str(x.shape))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"after flatten:\" + str(x.shape))\n",
    "\n",
    "        x = self.img_lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.img_lin2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #Label                                                  #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        y = self.label_lin1(label)\n",
    "        y = F.relu(y)\n",
    "        y = self.label_lin2(y)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        #Coordinate\n",
    "        z = self.coord_lin1(coord)\n",
    "        z = F.relu(z)\n",
    "        z = self.coord_lin2(z)\n",
    "        z = F.relu(z)\n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y,z],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "                                                                               #print(label) print(label.shape) print(coord) \n",
    "                                                                 #print(coord.shape) print(y) print(y.shape) #print(x.shape)\n",
    "        \n",
    "        x = self.dropout(concat)\n",
    "        x = self.comb_lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.comb_lin2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        \n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.img_lin1 = nn.Linear(z_dim, 256-9)\n",
    "        \n",
    "        self.lin1 = nn.Linear(256,512)\n",
    "        self.lin2 = nn.Linear(512,1024)\n",
    "        self.lin3 = nn.Linear(1024, init_kernel*(conv_out_size**2))\n",
    "        \n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*4, kernel_size=kernel_size, stride=2, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*2, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        \n",
    "    def forward(self, z, label, coord):\n",
    "        \n",
    "        x = self.img_lin1(z)\n",
    "        \n",
    "        #Label and Coordinate\n",
    "        #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        y = torch.cat([coord,label],dim=1)\n",
    "        x = torch.cat([x,y],dim=1)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin1(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin3(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_kernel, conv_out_size, conv_out_size)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dec1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self, image, label, coord):\n",
    "        mu, log_var = self.encoder(image, label, coord)\n",
    "        \n",
    "        #print('mu: ', mu.shape)\n",
    "        #print('log_var: ', log_var.shape)\n",
    "        \n",
    "        #sample z from latent distribution q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu,std)\n",
    "        z = q.rsample()\n",
    "        #print('z shape: ', z.shape)\n",
    "        \n",
    "        reconstruction = self.decoder(z, label, coord)\n",
    "                \n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(mean, logscale, sample):\n",
    "    scale = torch.exp(logscale)\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "    log_pxz = dist.log_prob(sample)\n",
    "    return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "def kl_divergence(z, mu, std):\n",
    "    # --------------------------\n",
    "    # Monte carlo KL divergence\n",
    "    # --------------------------\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the equation\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # kl\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True) #???\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        image, label, coord = batch\n",
    "        #print(image.size())\n",
    "        #print(label)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "        \n",
    "        #print(reconstruction.shape)\n",
    "        \n",
    "        #image = image.to(torch.device('cpu'))\n",
    "        recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "        \n",
    "        std = torch.exp(log_var / 2)\n",
    "        kl = kl_divergence(z, mu, std)\n",
    "\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "        \n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += elbo\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset) #Investigate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            \n",
    "            image, label, coord = batch\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                \n",
    "            reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "            \n",
    "            if (i == int(len(val_data)/dataloader.batch_size) - 1 and ( ((epoch%val_img_out_freq))==0) ): # or epoch > 90\n",
    "                num_rows = 4\n",
    "                both = torch.cat((image.view(batch_size, 3, 128, 128)[:4], \n",
    "                                  reconstruction.view(batch_size, 3, 128, 128)[:4]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "            \n",
    "            recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "            \n",
    "            std = torch.exp(log_var / 2)\n",
    "            kl = kl_divergence(z, mu, std)\n",
    "\n",
    "            elbo = (kl - recon_loss)\n",
    "            elbo = elbo.mean()\n",
    "            \n",
    "            running_loss += elbo \n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            mu, logvar = model.encoder(image.cuda(), label.cuda(), coord.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "#         print(len(latent))\n",
    "#         print(latent)\n",
    "#         print(len(target))\n",
    "#         print(target)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def run_each():\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        sleep(0.2)\n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        val_epoch_loss = validate(model, val_loader, epoch)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "\n",
    "#         print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    train_loss, val_loss = run_each()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    latent, target = generate_latent_vectors(model, val_loader)\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Latent\", \"Target\"])\n",
    "        \n",
    "        for i in range (0,len(latent)):\n",
    "            latent[i] = list(latent[i])\n",
    "            \n",
    "        wr.writerows(zip(latent, target))\n",
    "    \n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        \n",
    "        for i in range (0,len(train_loss)):\n",
    "            train_loss[i] = train_loss[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss)):\n",
    "            val_loss[i] = val_loss[i].item()\n",
    "            \n",
    "        wr.writerows(zip(train_loss, val_loss))\n",
    "        \n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter, value ,train_loss[-1], val_loss[-1], \n",
    "                     enc_out_dim, \n",
    "                     latent_dim, \n",
    "                     epochs,\n",
    "                     batch_size, \n",
    "                     learning_rate, \n",
    "                     kernel_size, \n",
    "                     stride,\n",
    "                     padding, \n",
    "                     init_kernel,\n",
    "                     dropout_pcent,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "    \"enc_out_dim\": 512,\n",
    "    \"latent_dim\": 128,\n",
    "    \"conv_out_size\": 64,\n",
    "    \n",
    "    \"epochs\" : 10,\n",
    "    \"batch_size\" : 8,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \n",
    "    \"kernel_size\" : 3,\n",
    "    \"stride\" : 1,\n",
    "    \"padding\" : 1,\n",
    "    \n",
    "    \"init_kernel\" : 8,\n",
    "    \n",
    "    \"dropout_pcent\": 0.4\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 188/188 [00:09<00:00, 19.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 99.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 118.76it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAJQCAYAAAAHVPnvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA+fElEQVR4nO3dfXxU5Z338e+ZTMgjSc6ZMxFBaEWkXSiaaJCHVqAQLQq2bNfqrcW+pGDxYe2GbNn6sLu+Xje4lSqgSCiWpXXtsqvuqnHtrnXFFNiauo319gGtIgJWDAjJTEJCEpJMzv1HklEKCXmcM2fm8/7HZB5/M5fg1+s61+8yHMdxBAAAAM/yuV0AAAAABodABwAA4HEEOgAAAI8j0AEAAHgcgQ4AAMDjCHQAAAAeR6ADAADwOL/bBbiturra7RI8z7Zt1dTUuF0GBoEx9DbGz/sYQ++LxRiOHj26x/uYoQMAAPA4Ah0AAIDHEegAAAA8LumvoQMAIFE5jqOWlhZ1dHTIMAy3y0lon3zyiU6cODHo13EcRz6fT+np6f0aMwIdAAAJqqWlRampqfL7+c/9cPP7/UpJSRmS12pvb1dLS4syMjL6/ByWXAEASFAdHR2EOQ/y+/3q6Ojo13MIdAAAJCiWWb2rv2NHbAcAAMMiFArp2muvlSQdPXpUKSkpsixLkvSf//mfGjFiRI/PfeONN/Tv//7vWrVqVZ/fb9q0aXr++eej75FMCHQAAGBYWJalF198UZK0du1aZWVl6eabb47e397e3uOS8IUXXqgLL7wwJnUmAgIdAACImZKSEuXl5Wn37t2aMmWKvv71r+uee+5RS0uL0tPTtW7dOk2YMEGVlZXavHmzHnvsMa1du1Yff/yx/vjHP+rjjz/WsmXLtHTp0j6938GDB1VaWqpQKCTLsrR+/XqNGTNGzz33nNavXy+fz6ecnBw9/fTTeu+991RaWqrW1lY5jqOf/vSnGj9+/DB/I0ODQAcAAGJq3759euKJJ5SSkqKGhgY9/fTT8vv92rVrl9asWaMtW7ac8py9e/fq3/7t33T8+HFdeuml+s53vqPU1NQzvtfdd9+tq6++Wtdcc40ef/xx/d3f/Z1+9rOf6cEHH9S2bdt09tlnq76+XpL0i1/8QkuXLtU3v/lNtba2KhKJDPlnHy4EOgAAkkDH41vkfLR/SF/TGHuufP/npn4/b+HChdEWH8eOHVNJSYn2798vwzDU1tZ22ufMmzdPaWlpSktLk23bOnr0aK9nm3b7/e9/r3/8x3+UJP3FX/yFVq9eLUkqKirSihUrdNVVV+mKK66QJF188cXasGGDDh06pCuuuMIzs3MSu1wBAECMZWZmRn++//77NXPmTFVUVOjRRx/tsTlvWlpa9OeUlJQBz5517x5ds2aN/uZv/kbV1dW6/PLLFQqF9Od//uf6+c9/rvT0dH3729/Wb37zmwG9hxuYoQMAIAkMZCYtFhoaGjRq1ChJ0pNPPjnkr19UVKRnn31WV199tZ5++mldcsklkqQDBw7ooosu0kUXXaQXX3xR1dXVamho0Oc+9zktXbpUH374of7whz/oK1/5ypDXNBwIdAAAwDW33HKLSkpK9NOf/lRf/vKXB/16xcXF0Vm4q666SqtWrVJpaak2b94c3RQhSatXr9b+/fvlOI6+8pWvaPLkydq4cWP0er78/HytWLFi0PXEiuE4juN2EW6qrq52uwTPs21bNTU1bpeBQWAMvY3x877hGsOmpqaTljcxfPx+v9rb24fs9U43dr1dM8g1dAAAAB5HoAMAAPA4Ah0AAIDHEegAAAA8jkAHAADgcQQ6AAAAjyPQDSPn7f+nyN3L5Rw55HYpAADE3NVXX60dO3acdNuWLVt055139vqcN954Q5J0ww03RM9Z/ay1a9dq8+bNvb73r371K+3Zsyf6+/33369du3b1o/rTq6ys1He+851Bv85QI9ANJ79fOnJIqj3idiUAAMTcN77xDT377LMn3fbss89q0aJFfXr+L37xC+Xm5g7ovf800K1cuVKzZs0a0Gt5AYFuOJm2JMkJ0fATAJB8FixYoO3bt0fPZ/3oo4/0ySef6JJLLtEdd9yhK664Ql/96lf1wAMPnPb506ZNUygUkiQ99NBDuvTSS3Xttdfqgw8+iD5m27ZtuvLKK1VcXKybbrpJzc3Nqqqq0osvvqjVq1frsssu04EDB1RSUqJf/vKXkqT/+Z//0eWXX6558+aptLQ0Wt+0adP0wAMP6Gtf+5rmzZunvXv39vmzPv3005o3b57mzp2re++9V5IUiURUUlKiuXPnat68efrpT38qSdq6davmzJmj4uJi3XLLLf38Vk+Po7+GU1egU/iou3UAAOACy7JUUFCgHTt26Gtf+5qeffZZff3rX5dhGPrhD38o0zQViUR07bXX6p133tGkSZNO+zpvvvmm/uM//kP//d//rfb2ds2fP18XXHCBJOmKK67Qt7/9bUnSmjVr9K//+q/67ne/q8suu0zFxcVauHDhSa/V0tKiFStW6IknntB5552n73//+3rsscd00003RWt+4YUX9Oijj2rz5s09hs3POnz4sFavXq3nn39eubm5uu666/SrX/1Ko0eP1uHDh1VRUSFJ0eXjsrIy/fa3v1VaWtppl5QHgkA3jIzUVGlkrsQMHQDAZf/46ifaH24Z0tc810zXsqKzen3MokWL9Oyzz0YD3bp16yRJzz33nLZt26ZIJKJPPvlE77//fo+B7n//9381f/58ZWRkSJIuu+yy6H3vvfeefvzjH+vYsWM6fvy4Zs+e3Ws9H3zwgcaNG6fzzjtPkvStb31L//RP/xQNdFdccYUk6YILLtDzzz/fh29BeuONNzRz5kwFAgFJ0je/+U298sorKikp0R//+Ef97d/+rebNmxet7c/+7M/0l3/5l5o/f77mz5/fp/c4E5Zch5sVlBMm0AEAktP8+fP1m9/8Rm+99ZZaWlo0ZcoU/fGPf9QjjzyiJ554Qtu3b9e8efPU0tJ72DQM47S3r1ixQqtXr9ZLL72kFStWRJdPe3KmI+zT0tIkSSkpKYpEIr0+9kyvmZeXpxdffFEzZszQo48+qh/84AeSpMcee0w33nij3nzzTc2fP39IzoBlhm64WbZ0+GO3qwAAJLkzzaQNl6ysLM2YMUOlpaXRzRANDQ3KyMhQTk6Ojh49ql//+teaMWNGj68xffp0rVixQrfddpsikYhefPFF3XDDDZKkxsZGnXXWWWpra9MzzzyjUaNGSZKys7N1/PjxU15rwoQJ+uijj7R//36de+65euqppzR9+vRBfcbCwkLdc889CoVCys3NVXl5ub773e8qFAopNTVVCxYs0Oc+9zmtWLFCHR0dqq6u1pe//GVdcsklKi8v1/Hjxwe8+aMbgW6YGVZQzh/ecLsMAABcs2jRIi1btkw/+clPJEmTJ0/Wl770JX31q1/VuHHjNHXq1F6fP2XKFF111VW6/PLLdc4552jatGnR+1auXKmFCxfqnHPO0Re/+EU1NjZK6txhu3LlSm3dujW6GUGS0tPTtW7dOi1fvlyRSEQXXnhhNBz21csvv6yLL744+vsjjzyiu+66S9/61rfkOI7mzp2rr33ta3r77bdVWlqqjo4OSdKdd96pSCSi22+/XQ0NDXIcRzfddNOgw5wkGc6Z5h4TXHV19bC+fscLz8j595/L99C/ysjMGtb3cott26qpYVnZyxhDb2P8vG+4xrCpqUmZmZlD/ro4ld/vH5Kl026nG7vRo0f3+HiuoRtuVvdOV/6yBQAAw4NAN8wMK9j5AztdAQDAMCHQDbdoc2F60QEAgOFBoBtueabk8zFDBwCIuSS/TN7T+jt2BLphZvhSpDyL0yIAADHn8/mG9EJ9xEZ7e7t8vv5FNNqWxIIV5DxXAEDMpaenq6WlRSdOnOixMS+GRlpa2hmbGveF4zjy+XxKT0/v1/MIdDFgmLacA++7XQYAIMkYhhE9LgvDy+32QSy5xoJlS+FarmUAAADDgkAXC2ZQam+TGurdrgQAACQgAl0MGDQXBgAAw4hAFws0FwYAAMOIQBcLFs2FAQDA8CHQxUJ2jpQ6ghk6AAAwLAh0MWAYhmQGuIYOAAAMCwJdrJg2S64AAGBYEOhixLCCLLkCAIBhQaCLFcuW6kJyIhG3KwEAAAmGQBcrli05HVJ9yO1KAABAgiHQxYhh0osOAAAMDwJdrHQ1F3bY6QoAAIYYgS5Wuo//YqcrAAAYYgS6GDEyMqWMTJZcAQDAkCPQxZJpyyHQAQCAIUagiyXL5rQIAAAw5Ah0MdTZXJhr6AAAwNAi0MWSaUsN9XLaWt2uBAAAJBACXSx173Rl2RUAAAwhAl0MGWZ36xICHQAAGDoEulgKdDUXJtABAIAhRKCLJZPmwgAAYOgR6GLISB0hjczlGjoAADCkCHSxRnNhAAAwxAh0sUZzYQAAMMQIdDFGc2EAADDUCHSxZtlSc5Oc5ia3KwEAAAmCQBdr9KIDAABDjEAXY0b0tAiWXQEAwNAg0MWaRXNhAAAwtAh0sZZrSYaPjREAAGDIEOhizEhJkfIsrqEDAABDhkDnBsuWQy86AAAwRAh0LjBMmxk6AAAwZAh0brCCUrhGjuO4XQkAAEgABDo3WLbU1io1HnO7EgAAkAAIdC4waC4MAACGEIHODTQXBgAAQ4hA5waaCwMAgCFEoHPDyFzJn0pzYQAAMCQIdC4wDEMyA1xDBwAAhgSBzi1WkObCAABgSBDoXEJzYQAAMFQIdG6xglJdrZyOiNuVAAAAjyPQucWypY4OqS7sdiUAAMDjCHQuMaK96Fh2BQAAg0Ogc0vXaRH0ogMAAINFoHNLV3NhTosAAACDRaBziZGZJaVnsNMVAAAMGoHOTaYth9MiAADAIBHo3GTRiw4AAAwegc5FhhVklysAABg0Ap2bLFs6Vienrc3tSgAAgIcR6Nxkdu90ZZYOAAAMHIHORTQXBgAAQ4FA5yaaCwMAgCFAoHNT9wwdrUsAAMAgEOhcZIxIk7JzaF0CAAAGxR+LN6mpqVFZWZnq6upkGIaKi4t15ZVX6sknn9RLL72knJwcSdJ1112niy66SJL0zDPPqKKiQj6fT0uWLFFBQYEkad++fSorK1Nra6sKCwu1ZMkSGYahtrY2bdy4Ufv27dPIkSNVUlKi/Pz8WHy8wbFsOVxDBwAABiEmgS4lJUU33HCDxo8fr+bmZt1xxx264IILJEkLFizQ17/+9ZMef/DgQVVWVmrdunUKh8NatWqVHnroIfl8Pm3ZskXLly/X+eefrx/96Ed6/fXXVVhYqIqKCmVlZenhhx/Wyy+/rG3btmnFihWx+HiDY9pSzSduVwEAADwsJkuupmlq/PjxkqSMjAyNGTNGoVCox8dXVVVp5syZSk1NVX5+vkaNGqW9e/cqHA6rublZEydOlGEYmjVrlqqqqiRJr776qubMmSNJmj59unbv3i3HcYb9sw2WYdnscgUAAIMS82vojhw5ov3792vChAmSpBdeeEE/+MEPtGnTJjU2NkqSQqGQAoFA9DmWZSkUCp1yeyAQiAbDz96XkpKizMxMNTQ0xOpjDZwVlJqOy2lpcrsSAADgUTFZcu3W0tKitWvX6sYbb1RmZqYuv/xyXX311ZKkJ554Qo899phuvfXWHmfWeptxO919hmGcctv27du1fft2SdJ9990n27YH8lGGTPPnztUxSaYTkd/lWgbK7/e7/j1icBhDb2P8vI8x9D63xzBmga69vV1r167VpZdeqmnTpkmS8vLyovfPmzdPa9askdQ581ZbWxu9LxQKybKsU26vra2VZVknPScQCCgSiaipqUnZ2dmn1FFcXKzi4uLo7zU17i53Ov50SVL4g/dlZIx0tZaBsm3b9e8Rg8MYehvj532MoffFYgxHjx7d430xWXJ1HEebN2/WmDFjtHDhwujt4XA4+vPvfvc7jR07VpJUVFSkyspKtbW16ciRIzp06JAmTJgg0zSVkZGhPXv2yHEc7dq1S0VFRZKkiy++WDt27JAkvfLKK5o8efJpZ+jiTlcvOna6AgCAgYrJDN17772nXbt2ady4cVq5cqWkzhYlL7/8sg4cOCDDMBQMBvW9731PkjR27FjNmDFDpaWl8vl8Wrp0qXy+zuy5bNkybdq0Sa2trSooKFBhYaEkae7cudq4caNuv/12ZWdnq6SkJBYfbfByLcnw0YsOAAAMmOF4YSvoMKqurna7BEVWLpExqUC+JX/ldikDwlKB9zGG3sb4eR9j6H1JseSKM6C5MAAAGAQCXRwwTJslVwAAMGAEunhg2VL4qCcaIQMAgPhDoIsHVlBqbZWOe6ARMgAAiDsEujhgdLUuUeiou4UAAABPItDFAzPY+U+uowMAAANAoIsHNBcGAACDQKCLByNzJb+fGToAADAgBLo4YPh8kmlzDR0AABgQAl28MG05zNABAIABINDFCcOyJa6hAwAAA0CgixemLdXVyumIuF0JAADwGAJdvLCCUiQiHatzuxIAAOAxBLo4EW0uXMvGCAAA0D8EunjRHei4jg4AAPQTgS5edJ0WwU5XAADQXwS6eJGZJaVlMEMHAAD6jUAXJwzDkCxbDs2FAQBAPxHo4olpc/wXAADoNwJdHKG5MAAAGAgCXTwxbelYnZz2NrcrAQAAHkKgiyeBoOQ4UrjW7UoAAICHEOjiiGF29aLjOjoAANAPBLp40tVc2Amz0xUAAPQdgS6edDUXZoYOAAD0B4EujhhpaVLWSHa6AgCAfiHQxRvLllPLkisAAOg7Al28sYLM0AEAgH4h0MUZg9MiAABAPxHo4o1lS02Nck60uF0JAADwCAJdvLHY6QoAAPqHQBdnPm0uzMYIAADQNwS6eNPdXJhABwAA+ohAF2/yApJhsNMVAAD0GYEuzhh+v5Rjcg0dAADoMwJdPLJsllwBAECfEejikWWz5AoAAPqMQBeHDDMohWrkOI7bpQAAAA8g0MUjy5ZaT0hNjW5XAgAAPIBAF4cMmgsDAIB+INDFI4vmwgAAoO8IdPHI7G4uzAwdAAA4MwJdPMrJk1L8UpgZOgAAcGYEujhk+HxSnsU1dAAAoE8IdPEqEKS5MAAA6BMCXZwyTJsZOgAA0CcEunhl2VJdSE5Hh9uVAACAOEegi1dmUIq0S8fq3K4EAADEOQJdnIo2F+ZMVwAAcAYEunhFc2EAANBHBLp4ZdFcGAAA9A2BLl5lZksj0tjpCgAAzohAF6cMw5AsWw6nRQAAgDMg0MUzK8gMHQAAOCMCXRyjuTAAAOgLAl08s2zpWFhOe5vblQAAgDhGoItnpi05jlQXcrsSAAAQxwh0ccwIdDUXZtkVAAD0gkAXz8zOQOfQXBgAAPSCQBfPuk+L4PgvAADQCwJdHDPS0jsbDLPkCgAAekGgi3eWLYcZOgAA0AsCXbyzglIt19ABAICeEejinGHZXEMHAAB6RaCLd6YtHW+Qc+KE25UAAIA4RaCLd9Gdriy7AgCA0yPQxTnDorkwAADoHYEu3pmdM3Q0FwYAAD0h0MU7MyAZBjN0AACgRwS6OGf4U6WcPHa6AgCAHhHovMC05TBDBwAAekCg8wIrKHENHQAA6AGBzgO6mws7juN2KQAAIA4R6LzAtKUTLVLTcbcrAQAAcYhA5wEGzYUBAEAvCHReQHNhAADQCwKdF1g0FwYAAD0j0HlBTp6UksIMHQAAOC0CnQcYvhQpL0BzYQAAcFoEOq+guTAAAOgBgc4jDJoLAwCAHhDovMKypXCtnI4OtysBAABxhkDnFZYtRdqlhnq3KwEAAHGGQOcRhtnVXJjr6AAAwJ8g0HlFd3NhTosAAAB/gkDnFTQXBgAAPSDQeUXWSGnECJZcAQDAKQh0HmEYhmQGCXQAAOAUBDovsWw5nBYBAAD+BIHOQwzLprkwAAA4BYHOS8ygVB+W097udiUAACCOEOi8xLIlx5HqQ25XAgAA4giBzkNoLgwAAE6HQOclgc7mwvSiAwAAn0Wg8xJm6AAAwGkQ6DzESM+QMrM4/gsAAJyEQOc1pi2HGToAAPAZBDqvsYISzYUBAMBn+GPxJjU1NSorK1NdXZ0Mw1BxcbGuvPJKNTY2av369Tp69KiCwaBWrFih7OxsSdIzzzyjiooK+Xw+LVmyRAUFBZKkffv2qaysTK2trSosLNSSJUtkGIba2tq0ceNG7du3TyNHjlRJSYny8/Nj8fFiyrBsOfvfc7sMAAAQR2IyQ5eSkqIbbrhB69ev17333qsXXnhBBw8eVHl5uaZMmaINGzZoypQpKi8vlyQdPHhQlZWVWrdune6++25t3bpVHR0dkqQtW7Zo+fLl2rBhgw4fPqzXX39dklRRUaGsrCw9/PDDWrBggbZt2xaLjxZ7pi01Nsg5ccLtSgAAQJyISaAzTVPjx4+XJGVkZGjMmDEKhUKqqqrS7NmzJUmzZ89WVVWVJKmqqkozZ85Uamqq8vPzNWrUKO3du1fhcFjNzc2aOHGiDMPQrFmzos959dVXNWfOHEnS9OnTtXv3bjmOE4uPF1tWZ+sSll0BAEC3mF9Dd+TIEe3fv18TJkxQfX29TNOU1Bn6jh07JkkKhUIKBALR51iWpVAodMrtgUBAoVDolOekpKQoMzNTDQ0NsfpYMWNYXa1LCHQAAKBLTK6h69bS0qK1a9fqxhtvVGZmZo+P62lmrbcZt9PdZxjGKbdt375d27dvlyTdd999sm37TGXHlfb2iaqVlN3Woow4qd3v93vue8TJGENvY/y8jzH0PrfHMGaBrr29XWvXrtWll16qadOmSZJyc3MVDodlmqbC4bBycnIkdc681dbWRp8bCoVkWdYpt9fW1sqyrJOeEwgEFIlE1NTUFN1g8VnFxcUqLi6O/l5T462ZLsfpnFRt+HC/jsdJ7bZte+57xMkYQ29j/LyPMfS+WIzh6NGje7wvJkuujuNo8+bNGjNmjBYuXBi9vaioSDt37pQk7dy5U1OnTo3eXllZqba2Nh05ckSHDh3ShAkTZJqmMjIytGfPHjmOo127dqmoqEiSdPHFF2vHjh2SpFdeeUWTJ08+7Qyd1xmpqVJOHkuuAAAgKiYzdO+995527dqlcePGaeXKlZKk6667TosWLdL69etVUVEh27ZVWloqSRo7dqxmzJih0tJS+Xw+LV26VD5fZ/ZctmyZNm3apNbWVhUUFKiwsFCSNHfuXG3cuFG33367srOzVVJSEouP5g7T5jxXAAAQZTgJuRW076qrq90uod8im/5BOvyxUv5vmdulSGKpIBEwht7G+HkfY+h9SbHkiqFlWEEpVJOYbVkAAEC/Eei8yLKlE81S83G3KwEAAHGAQOdFZldz4RDT8wAAgEDnSTQXBgAAn0Wg8yKzM9A5zNABAAAR6Lwpz5R8PonWJQAAQAQ6TzJ8KVJegGvoAACAJAKdd1m2HK6hAwAAItB5lmHaLLkCAABJBDrvsoJSuEZOR4fblQAAAJcR6LzKsqX2dqmx3u1KAACAywh0HhXtRcfGCAAAkh6Bzqs4LQIAAHQh0HlV1wwdO10BAACBzquyc6TUEex0BQAABDqvMgyj8wgwllwBAEh6BDovo7kwAAAQgc7TDGboAACACHTeFghKdSE5kYjblQAAABcR6LzMtCWnQ6oLuV0JAABwEYHOw6LNhcPsdAUAIJkR6Lysq7mww3V0AAAkNQKdl0Vn6Ah0AAAkMwKdhxkZmVJGllTLkisAAMmMQOd19KIDACDpEei8jl50AAAkPQKdxxmWzTV0AAAkOQKd11lBqaFeTusJtysBAAAuIdB5ndm907XW3ToAAIBrCHQeF20uHGKnKwAAyYpA53VdgY6drgAAJC8Cndd1L7my0xUAgKRFoPM4I3WENDKXJVcAAJIYgS4RWEGWXAEASGIEukRAc2EAAJIagS4B0FwYAIDkRqBLBFZQam6S03Tc7UoAAIALCHSJoLsXHbN0AAAkJQJdAjBoXQIAQFIj0CWCaHNhWpcAAJCMCHSJINeSDB8zdAAAJCkCXQIwUlIk06K5MAAASYpAlyhMWw4zdAAAJCUCXYIwrCC7XAEASFIEukRh2lK4Vo7juF0JAACIMQJdorCCUlur1HjM7UoAAECMEegShNHdXJiNEQAAJJ0+B7rdu3fryJEjkqRwOKyNGzdq06ZNqqurG67a0B8WzYUBAEhWfQ50W7dulc/X+fDHHntMkUhEhmHokUceGbbi0A9dp0Ww0xUAgOTj7+sDQ6GQbNtWJBLRG2+8oU2bNsnv92v58uXDWR/6amSu5E+VOC0CAICk0+dAl5GRobq6On300Uc655xzlJ6ervb2drW3tw9nfegjwzA6l12ZoQMAIOn0OdDNnz9fd955p9rb23XjjTdKkt59912NGTNmuGpDf5m2HDZFAACQdPoc6BYtWqRLLrlEPp9Po0aNkiRZlqWbb7552IpD/xiWLee9t9wuAwAAxFifA50kjR49Ovrz7t275fP5NGnSpCEvCgNkBqW6kJyOiAxfitvVAACAGOnzLtd77rlH7777riSpvLxcDz30kB566CE9/fTTw1Yc+ilgSx0dUl3Y7UoAAEAM9TnQffTRR5o4caIk6aWXXtI999yje++9Vy+++OKwFYf+Mcxg5w9cRwcAQFLp85Jr9xmhhw8fliSdc845kqTjx48PQ1kYkK7mwk64RobLpQAAgNjpc6D7whe+oJ/97GcKh8OaOnWqpM5wN3LkyGErDv1kcloEAADJqM9LrrfddpsyMzP1uc99Ttdcc40kqbq6WldeeeWwFYf+MTKzpPQMKUygAwAgmfR5hm7kyJG6/vrrT7rtoosuGvKCMEhWUE4t19ABAJBM+hzo2tvb9fTTT2vXrl0Kh8MyTVOzZs3SN7/5Tfn9/ep+guFk2czQAQCQZPqcxP75n/9ZH3zwgW666SYFg0EdPXpUTz31lJqamqInR8B9hmnL+fADt8sAAAAx1OdA98orr+j++++PboIYPXq0zj33XK1cuZJAF08sW2qol9PWJiM11e1qAABADPR5U0R32xLEOaurFx3LrgAAJI0+z9DNmDFDa9as0dVXXy3btlVTU6OnnnpKM2bMGM760E+GacuROpsL55/tdjkAACAG+hzoFi9erKeeekpbt25VOByWZVmaOXOm2tvbh7M+9FfXDJ0TorkwAADJos+Bzu/369prr9W1114bva21tVU33HCDFi9ePCzFYQDMQOc/WXIFACBp9PkautMxDOaA4o0xIk3KzuG0CAAAksigAh3ilBWUE6K5MAAAyeKMS667d+/u8T6un4tTli0dPex2FQAAIEbOGOh+8pOf9Hq/bdtDVgyGhmHact7rOYgDAIDEcsZAV1ZWFos6MJQsW2o+LqelSUZ6ptvVAACAYcY1dImou7kwGyMAAEgKBLoEZFhdy+BsjAAAICkQ6BKR+WlzYQAAkPgIdIkoz5IMH82FAQBIEgS6BGSkpEi5JtfQAQCQJAh0iSpAc2EAAJIFgS5BGabNDB0AAEmCQJeoLFsK18hxHLcrAQAAw4xAl6hMW2prlRob3K4EAAAMMwJdgjK6mwuHuY4OAIBER6BLVDQXBgAgaRDoElVXoKO5MAAAiY9Al6iycyW/n52uAAAkAQJdgjJ8vs6NEZwWAQBAwiPQJTKL5sIAACQDAl0Co7kwAADJgUCXyCxbqquV0xFxuxIAADCMCHSJzLSljg6pvs7tSgAAwDAi0CUwg150AAAkBQJdIus6LYJedAAAJDYCXSLrnqHj+C8AABIagS6RZWRJaRnsdAUAIMER6BKYYRiSZcuhuTAAAAmNQJfoLFuqZckVAIBERqBLcIYV5PgvAAASHIEu0Zm2dKxOTlub25UAAIBhQqBLdN07Xetq3a0DAAAMG38s3mTTpk167bXXlJubq7Vr10qSnnzySb300kvKycmRJF133XW66KKLJEnPPPOMKioq5PP5tGTJEhUUFEiS9u3bp7KyMrW2tqqwsFBLliyRYRhqa2vTxo0btW/fPo0cOVIlJSXKz8+PxUeLe4Zpy5E6d7oGR7ldDgAAGAYxmaGbM2eO7rrrrlNuX7Bgge6//37df//90TB38OBBVVZWat26dbr77ru1detWdXR0SJK2bNmi5cuXa8OGDTp8+LBef/11SVJFRYWysrL08MMPa8GCBdq2bVssPpY3RJsLszECAIBEFZNAN2nSJGVnZ/fpsVVVVZo5c6ZSU1OVn5+vUaNGae/evQqHw2pubtbEiRNlGIZmzZqlqqoqSdKrr76qOXPmSJKmT5+u3bt3y3Gc4fo43mJy/BcAAIkuJkuuPXnhhRe0a9cujR8/Xt/5zneUnZ2tUCik888/P/oYy7IUCoWUkpKiQCAQvT0QCCgUCkmSQqFQ9L6UlBRlZmaqoaEhupybzIy0NCl7JDtdAQBIYK4Fussvv1xXX321JOmJJ57QY489pltvvbXHmbXeZtxOd59hGKd97Pbt27V9+3ZJ0n333SfbtvtbuufUBkfJ13hM5jB9Vr/fnxTfYyJjDL2N8fM+xtD73B5D1wJdXl5e9Od58+ZpzZo1kjpn3mprP92RGQqFZFnWKbfX1tbKsqyTnhMIBBSJRNTU1NTjEm9xcbGKi4ujv9fUJP7MVSTHlA5/PGyf1bbtpPgeExlj6G2Mn/cxht4XizEcPXp0j/e51rYkHA5Hf/7d736nsWPHSpKKiopUWVmptrY2HTlyRIcOHdKECRNkmqYyMjK0Z88eOY6jXbt2qaioSJJ08cUXa8eOHZKkV155RZMnT+5xhi4ZGZbNkisAAAksJjN0Dz74oN555x01NDTo5ptv1jXXXKO3335bBw4ckGEYCgaD+t73vidJGjt2rGbMmKHS0lL5fD4tXbpUPl9n7ly2bJk2bdqk1tZWFRQUqLCwUJI0d+5cbdy4Ubfffruys7NVUlISi4/lHWZQajoup6VZRnqG29UAAIAhZjhJvh20urra7RKGXcf/7pTzj2vl+79lMs4eO+Svz1KB9zGG3sb4eR9j6H1Ju+SK2DGirUv4ywIAgEREoEsGAZoLAwCQyAh0ySDXkgyDGToAABIUgS4JGH6/lGtKYWboAABIRAS6ZGHacpihAwAgIRHokoRhBVlyBQAgQRHokoVlS+GjvR6hBgAAvIlAlywsW2ptlY43uF0JAAAYYgS6JGGYna1LWHYFACDxEOiShdXVXJgzXQEASDgEumRh0VwYAIBERaBLFiNzpRQ/S64AACQgAl2SMHw+yQwQ6AAASEAEumRi2XI4LQIAgIRDoEsiNBcGACAxEeiSiWlLdbVyOiJuVwIAAIYQgS6ZWLYUiUjH6tyuBAAADCECXRKhuTAAAImJQJdMaC4MAEBCItAlk+7mwrXsdAUAIJEQ6JJJZpaUls4MHQAACYZAl0QMw5BMWw7X0AEAkFAIdMnGspmhAwAgwRDokkxnc2GuoQMAIJEQ6JKNaUvH6uS0t7ldCQAAGCIEumRj2ZLjSOFatysBAABDhECXZAx60QEAkHAIdMmm67QIdroCAJA4CHTJpnuGjo0RAAAkDAJdkjHS0qWskSy5AgCQQAh0yYjmwgAAJBQCXTKybIlABwBAwiDQJSGaCwMAkFgIdMnIsqWmRjknWtyuBAAADAECXTIyu3e6suwKAEAiINAloU+bC7PsCgBAIiDQJaOuGTp2ugIAkBgIdMnIDEiGwcYIAAASBIEuCRn+VCnH5Bo6AAASBIEuWVm2HE6LAAAgIRDokpVJc2EAABIFgS5JdTcXdhzH7VIAAMAgEeiSlWVLrSekpka3KwEAAINEoEtS0V50LLsCAOB5BLpkxWkRAAAkDAJdsuqaoXM4LQIAAM8j0CWrHFNK8dNcGACABECgS1KGzyflWSy5AgCQAAh0yYzmwgAAJAQCXRIzzCAzdAAAJAACXTIL2FK4Vk5Hh9uVAACAQSDQJTMzKEXapWN1blcCAAAGgUCXxKLNhbmODgAATyPQJTOaCwMAkBAIdMmM5sIAACQEAl0yyxopjUiTapmhAwDAywh0ScwwjK5edMzQAQDgZQS6ZGfaXEMHAIDHEeiSnGHZ7HIFAMDjCHTJzgpK9WE57W1uVwIAAAaIQJfsTFtyHKku5HYlAABggAh0Sc6wgp0/cB0dAACeRaBLdtFedAQ6AAC8ikCX7DgtAgAAzyPQJTkjPUPKzJZC9KIDAMCrCHToai7MDB0AAF5FoENXc2Fm6AAA8CoCHWguDACAxxHo0DlD19gg58QJtysBAAADQKCDFOjqRRdm2RUAAC8i0EGGSXNhAAC8jEAHmgsDAOBxBDpIeYHOfzJDBwCAJxHoICM1Vco1aV0CAIBHEejQybTlMEMHAIAnEejQiV50AAB4FoEOkiTDtKVQjRzHcbsUAADQTwQ6dLJs6USz1HTc7UoAAEA/EeggSTIsmgsDAOBVBDp0Mjt70dG6BAAA7yHQoVPXDB07XQEA8B4CHTrl5kkpKex0BQDAgwh0kCQZvpTOEyNoLgwAgOcQ6PApmgsDAOBJBDpEGTQXBgDAkwh0+JTZGeicjg63KwEAAP1AoMOnLFtqb5ca692uBAAA9AOBDlHR5sK1LLsCAOAlBDp8yupqLsxpEQAAeAqBDp8yaS4MAIAXEejwqeyRUuoIdroCAOAxBDpEGYbReQRYLUuuAAB4CYEOJ7NsOczQAQDgKQQ6nMQwbYlr6AAA8BQCHU5m2VJ9WE4k4nYlAACgjwh0OJlpS06HVBdyuxIAANBHBDqcJNpcOMTGCAAAvIJAh5N1NRd2CHQAAHgGgQ4ni54WwcYIAAC8wh+LN9m0aZNee+015ebmau3atZKkxsZGrV+/XkePHlUwGNSKFSuUnZ0tSXrmmWdUUVEhn8+nJUuWqKCgQJK0b98+lZWVqbW1VYWFhVqyZIkMw1BbW5s2btyoffv2aeTIkSopKVF+fn4sPlrCMdIzpYwsdroCAOAhMZmhmzNnju66666TbisvL9eUKVO0YcMGTZkyReXl5ZKkgwcPqrKyUuvWrdPdd9+trVu3qqOjQ5K0ZcsWLV++XBs2bNDhw4f1+uuvS5IqKiqUlZWlhx9+WAsWLNC2bdti8bESl2Wz5AoAgIfEJNBNmjQpOvvWraqqSrNnz5YkzZ49W1VVVdHbZ86cqdTUVOXn52vUqFHau3evwuGwmpubNXHiRBmGoVmzZkWf8+qrr2rOnDmSpOnTp2v37t1yHCcWHy0xWUGWXAEA8BDXrqGrr6+XaZqSJNM0dezYMUlSKBRSIBCIPs6yLIVCoVNuDwQCCoVCpzwnJSVFmZmZamhoiNVHSTg0FwYAwFticg1df/Q0s9bbjNvp7jMM47SP3b59u7Zv3y5Juu+++2Tb9gCqTGyN54zT8V2/UmDkSBlpaWd8vN/v53v0OMbQ2xg/72MMvc/tMXQt0OXm5iocDss0TYXDYeXk5EjqnHmrra2NPi4UCsmyrFNur62tlWVZJz0nEAgoEomoqanplCXebsXFxSouLo7+XlPDTNSf6kjLlCTV7H1Pxlmjz/h427b5Hj2OMfQ2xs/7GEPvi8UYjh7d83+TXVtyLSoq0s6dOyVJO3fu1NSpU6O3V1ZWqq2tTUeOHNGhQ4c0YcIEmaapjIwM7dmzR47jaNeuXSoqKpIkXXzxxdqxY4ck6ZVXXtHkyZN7nKHDmRkBmgsDAOAlMZmhe/DBB/XOO++ooaFBN998s6655hotWrRI69evV0VFhWzbVmlpqSRp7NixmjFjhkpLS+Xz+bR06VL5fJ25c9myZdq0aZNaW1tVUFCgwsJCSdLcuXO1ceNG3X777crOzlZJSUksPlbiMrubC9eIWAwAQPwznCTfDlpdXe12CXHHaWtTx61/IeMb18u38P+c8fEsFXgfY+htjJ/3MYbel7RLrohfRmqqNDKXna4AAHgEgQ6nZwVpLgwAgEcQ6HB6Fr3oAADwCgIdTsvgtAgAADyDQIfTM22ppVlO03G3KwEAAGdAoMPpWV3drpmlAwAg7hHocFqGRXNhAAC8gkCH0/tMc2EAABDfCHQ4vTxT8vnY6QoAgAcQ6HBahi9FyrOkMEuuAADEOwIdemYFWXIFAMADCHTokWHabIoAAMADCHTomWVL4Vo5juN2JQAAoBcEOvTMDErtbVJDvduVAACAXhDo0COD5sIAAHgCgQ49624uXMt1dAAAxDMCHXrWNUPnMEMHAEBcI9ChZ9k5UuoImgsDABDnCHTokWEYkhngGjoAAOIcgQ69s4Jy6EUHAEBcI9ChV53NhZmhAwAgnhHo0DvLlupCciIRtysBAAA9INChd5YtOR1SfcjtSgAAQA8IdOiVYXb1omPZFQCAuEWgQ++6mguzMQIAgPhFoEPvOP4LAIC4R6BDr4yMTCkjkyVXAADiGIEOZ2bacgh0AADELQIdzswKSlxDBwBA3CLQ4YwMy+YaOgAA4hiBDmdm2lJDvZy2VrcrAQAAp0Ggw5mx0xUAgLhGoMMZGWZXoGNjBAAAcYlAhzML0FwYAIB4RqDDmTFDBwBAXCPQ4YyM1BHSyFyuoQMAIE4R6NA3NBcGACBuEejQNzQXBgAgbhHo0Cc0FwYAIH4R6NA3li01N8lpbnK7EgAA8CcIdOgbdroCABC3CHToEyN6WgTX0QEAEG8IdOgbi+bCAADEKwId+ibXkgwfS64AAMQhAh36xEhJkfIsAh0AAHGIQIe+s2w5tC4BACDuEOjQZ4Zp01wYAIA4RKBD31lBKVwrx3HcrgQAAHwGgQ59Z9lSW6vUeMztSgAAwGcQ6NBnBs2FAQCISwQ69B3NhQEAiEsEOvRdd3PhWmboAACIJwQ69N3IXMmfygwdAABxhkCHPjMMQzIDXEMHAECcIdChf6wgzYUBAIgzBDr0C82FAQCIPwQ69I8VlOpCcjoiblcCAAC6EOjQP5YtdXRIdWG3KwEAAF0IdOgXI9qLjuvoAACIFwQ69E/XaREOO10BAIgbBDr0T1dzYTZGAAAQPwh06BcjM0tKz2DJFQCAOEKgQ/+Zthxm6AAAiBsEOvSfZXNaBAAAcYRAh34zrCDX0AEAEEcIdOg/y5Ya6uW0tbldCQAAEIEOA2F27XRlYwQAAHGBQId+o7kwAADxhUCH/qO5MAAAcYVAh/7rnqFjYwQAAHGBQId+M0akSdk5tC4BACBOEOgwMJYth2voAACICwQ6DIxps+QKAECcINBhQAxOiwAAIG4Q6DAwVlBqPi6npcntSgAASHoEOgyM2b3TlVk6AADcRqDDgBhW12kRBDoAAFxHoMPAdPWiY6crAADuI9BhYHItyfCx0xUAgDhAoMOAGH6/lGuy5AoAQBwg0GHgaC4MAEBcINBhwAyTXnQAAMQDAh0Gzuo8LcJxHLcrAQAgqRHoMHBWUGprldNQ73YlAAAkNQIdBszoal0SqfnE5UoAAEhuBDoMnNnZXDhSc8TlQgAASG5+twuAh3XP0B3+WM6Y8S4XM8wMtwsYXk7rCTltrTF8xxh/oYk+fm1tctrb3C5jmCX2IDqRdjmRiNtlDK/EHkI5HR2uvr/hJPkV7dXV1cP22n840qTH33JhF6gRoz81jiP94Q3Jcfdf4lgwkvuPyZCL/d/rjF9/DNX4ePnPjRHjf2f6/X79eHh/Xrv/Y9+P147xvw6xHsPLJ41S4ddmD+t7jB49usf7mKEbRh2O1Nwe67/QYvt+zujx8kXa1dGRuP9n6eH/JvWZz+dTR4z+7zIJvs6YG8rxG6rxGdpxju3/AsT8b21n+P4M9uez9Pdzx/OfZceF/21sPMuM+Xt+FjN0wzhDlyxs21ZNDf3ovIwx9DbGz/sYQ++LxRj2NkPHpggAAACPI9ABAAB4HIEOAADA4wh0AAAAHkegAwAA8DgCHQAAgMcR6AAAADyOQAcAAOBxrp8Ucdtttyk9PV0+n08pKSm677771NjYqPXr1+vo0aMKBoNasWKFsrOzJUnPPPOMKioq5PP5tGTJEhUUFEiS9u3bp7KyMrW2tqqwsFBLliyREasjsAAAAFzkeqCTpHvuuUc5OTnR38vLyzVlyhQtWrRI5eXlKi8v1+LFi3Xw4EFVVlZq3bp1CofDWrVqlR566CH5fD5t2bJFy5cv1/nnn68f/ehHev3111VYWOjipwIAAIiNuFxyraqq0uzZnQfczp49W1VVVdHbZ86cqdTUVOXn52vUqFHau3evwuGwmpubNXHiRBmGoVmzZkWfAwAAkOjiYobu3nvvlSRddtllKi4uVn19vUyz85Bb0zR17NgxSVIoFNL5558ffZ5lWQqFQkpJSVEgEIjeHggEFAqFYvgJAAAA3ON6oFu1apUsy1J9fb1Wr17d68GzjuP06/bT2b59u7Zv3y5Juu+++2Tbdv8Kxin8fj/fo8cxht7G+HkfY+h9bo+h64HOsixJUm5urqZOnaq9e/cqNzdX4XBYpmkqHA5Hr68LBAKqra2NPjcUCsmyrFNur62tjb7unyouLlZxcXH095qamuH4WEnFtm2+R49jDL2N8fM+xtD7YjGGvU16uXoNXUtLi5qbm6M/v/nmmxo3bpyKioq0c+dOSdLOnTs1depUSVJRUZEqKyvV1tamI0eO6NChQ5owYYJM01RGRob27Nkjx3G0a9cuFRUVufa5AAAAYsnVGbr6+no98MADkqRIJKKvfOUrKigo0Hnnnaf169eroqJCtm2rtLRUkjR27FjNmDFDpaWl8vl8Wrp0qXy+zky6bNkybdq0Sa2trSooKGCHKwAASBqG058L0BJQdXW12yV4HksF3scYehvj532Mofcl9ZIrAAAABo9ABwAA4HEEOgAAAI8j0AEAAHgcgQ4AAMDjCHQAAAAel/RtSwAAALyOGToM2h133OF2CRgkxtDbGD/vYwy9z+0xJNABAAB4HIEOAADA4wh0GLTi4mK3S8AgMYbexvh5H2PofW6PIZsiAAAAPI4ZOgAAAI/zu10AvKmmpkZlZWWqq6uTYRgqLi7WlVde6XZZGICOjg7dcccdsizL9V1a6L/jx49r8+bN+uijj2QYhm655RZNnDjR7bLQR7/85S9VUVEhwzA0duxY3XrrrRoxYoTbZaEXmzZt0muvvabc3FytXbtWktTY2Kj169fr6NGjCgaDWrFihbKzs2NaF4EOA5KSkqIbbrhB48ePV3Nzs+644w5dcMEFOuecc9wuDf30X//1XxozZoyam5vdLgUD8POf/1wFBQX667/+a7W3t+vEiRNul4Q+CoVCev7557V+/XqNGDFC69atU2VlpebMmeN2aejFnDlzNH/+fJWVlUVvKy8v15QpU7Ro0SKVl5ervLxcixcvjmldLLliQEzT1Pjx4yVJGRkZGjNmjEKhkMtVob9qa2v12muvad68eW6XggFoamrSH/7wB82dO1eS5Pf7lZWV5XJV6I+Ojg61trYqEomotbVVpmm6XRLOYNKkSafMvlVVVWn27NmSpNmzZ6uqqirmdTFDh0E7cuSI9u/frwkTJrhdCvrp0Ucf1eLFi5md86gjR44oJydHmzZt0ocffqjx48frxhtvVHp6utuloQ8sy9JVV12lW265RSNGjNCFF16oCy+80O2yMAD19fXRMG6apo4dOxbzGpihw6C0tLRo7dq1uvHGG5WZmel2OeiH3//+98rNzY3OtMJ7IpGI9u/fr8svv1w//vGPlZaWpvLycrfLQh81NjaqqqpKZWVleuSRR9TS0qJdu3a5XRY8ikCHAWtvb9fatWt16aWXatq0aW6Xg35677339Oqrr+q2227Tgw8+qN27d2vDhg1ul4V+CAQCCgQCOv/88yVJ06dP1/79+12uCn311ltvKT8/Xzk5OfL7/Zo2bZr27NnjdlkYgNzcXIXDYUlSOBxWTk5OzGtgyRUD4jiONm/erDFjxmjhwoVul4MBuP7663X99ddLkt5++20999xz+v73v+9yVeiPvLw8BQIBVVdXa/To0XrrrbfYmOQhtm3r/fff14kTJzRixAi99dZbOu+889wuCwNQVFSknTt3atGiRdq5c6emTp0a8xpoLIwBeffdd/X3f//3GjdunAzDkCRdd911uuiii1yuDAPRHehoW+I9Bw4c0ObNm9Xe3q78/HzdeuutMW+XgIF78sknVVlZqZSUFH3+85/XzTffrNTUVLfLQi8efPBBvfPOO2poaFBubq6uueYaTZ06VevXr1dNTY1s21ZpaWnM/xwS6AAAADyOa+gAAAA8jkAHAADgcQQ6AAAAjyPQAQAAeByBDgAAwOMIdAAQY9dcc40OHz7sdhkAEgiNhQEkvdtuu011dXXy+T79f9w5c+Zo6dKlLlYFAH1HoAMAST/84Q91wQUXuF0GAAwIgQ4AerBjxw699NJLOvfcc7Vz506ZpqmlS5dqypQpkqRQKKQtW7bo3XffVXZ2tr7xjW+ouLhYktTR0aHy8nL9+te/Vn19vc4++2ytXLlStm1Lkt588039wz/8gxoaGvTlL39ZS5culWEYOnz4sH7yk5/owIED8vv9+tKXvqQVK1a49h0A8AYCHQD04v3339e0adO0detW/e53v9MDDzygsrIyZWdn66GHHtLYsWP1yCOPqLq6WqtWrdJZZ52lKVOm6Je//KVefvll3XnnnTr77LP14YcfKi0tLfq6r732mn70ox+publZP/zhD1VUVKSCggI9/vjjuvDCC3XPPfeovb1d+/btc/HTA/AKAh0ASLr//vuVkpIS/X3x4sXy+/3Kzc3VggULZBiGZs6cqeeee06vvfaaJk2apHfffVd33HGHRowYoc9//vOaN2+edu3apSlTpuill17S4sWLNXr0aEnS5z//+ZPeb9GiRcrKylJWVpYmT56sAwcOqKCgQH6/X0ePHlU4HFYgENAXv/jFWH4NADyKQAcAklauXHnKNXQ7duyQZVkyDCN6WzAYVCgUUjgcVnZ2tjIyMqL32batDz74QJJUW1urs846q8f3y8vLi/6clpamlpYWSZ1B8vHHH9ddd92lrKwsLVy4UHPnzh2KjwgggRHoAKAXoVBIjuNEQ11NTY2KiopkmqYaGxvV3NwcDXU1NTWyLEuSFAgE9Mknn2jcuHH9er+8vDzdfPPNkqR3331Xq1at0qRJkzRq1Kgh/FQAEg196ACgF/X19Xr++efV3t6u3/72t/r4449VWFgo27b1hS98Qf/yL/+i1tZWffjhh/r1r3+tSy+9VJI0b948PfHEEzp06JAcx9GHH36ohoaGM77fb3/7W9XW1kqSsrKyJOmkdioAcDrM0AGApDVr1pwUnC644AJNnTpV559/vg4dOqSlS5cqLy9PpaWlGjlypCTpr/7qr7RlyxYtX75c2dnZ+ta3vhVdtl24cKHa2tq0evVqNTQ0aMyYMfrBD35wxjo++OADPfroo2pqalJeXp6WLFmi/Pz84fnQABKG4TiO43YRABCPutuWrFq1yu1SAKBXzOMDAAB4HIEOAADA41hyBQAA8Dhm6AAAADyOQAcAAOBxBDoAAACPI9ABAAB4HIEOAADA4wh0AAAAHvf/Aa8Q2QnRyGv7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_img_out_freq = 5\n",
    "parameter_list = ['LayerExpansionTest']\n",
    "value_list = [0]\n",
    "\n",
    "for parameter in parameter_list:\n",
    "    for value in value_list:\n",
    "        #Update the values to be updated and rerun the experiment\n",
    "        enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "        latent_dim = param_dict[\"latent_dim\"]\n",
    "        conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "        epochs = param_dict[\"epochs\"]\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        learning_rate = param_dict[\"learning_rate\"]\n",
    "\n",
    "        kernel_size = param_dict[\"kernel_size\"]\n",
    "        stride = param_dict[\"stride\"]\n",
    "        padding = param_dict[\"padding\"]\n",
    "        init_kernel = param_dict[\"init_kernel\"]\n",
    "        \n",
    "        dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "        \n",
    "        #Move batch_size to before so its trained on the same split?\n",
    "        set_used = 'datasets/SmallGrey'\n",
    "        train_data = ActiveVisionDataset(csv_file=set_used+'/TrainSet/rgbCSV.csv', root_dir=set_used+'/TrainSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        img_size = len(train_data[0][0][0])\n",
    "        model = ConditionalVAE(latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0])).to(device)\n",
    "        \n",
    "        \n",
    "        #Change the value to a string for later\n",
    "        value = str(value)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "        \n",
    "        #Run the Test\n",
    "        runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

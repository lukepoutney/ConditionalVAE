{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAE\n",
    "\n",
    "With Input:\n",
    "- Image Label\n",
    "- Coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        #image = image/(image.max()/255.0)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #print(shape_label)\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "#         print(cam_loc)\n",
    "        cam_loc = torch.tensor([cam_loc[1]/(2*math.pi), cam_loc[2]/math.pi])\n",
    "#         print(cam_loc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        self.img_lin1 = nn.Linear(init_filters*(conv_out_size**2), 1024)\n",
    "        self.img_lin2 = nn.Linear(4096, 1024)\n",
    "        \n",
    "        self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "        self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(1024+2+6,256)\n",
    "        self.comb_lin2 = nn.Linear(512,256)\n",
    "\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.sigma = nn.Linear(256, z_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        self.dropout2D = nn.Dropout2d(p=dropout_pcent)\n",
    "        \n",
    "        self.dropout_label = nn.Dropout(p=label_dropout_pcent)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(1024+2+6)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, image, label, coord):\n",
    "        \n",
    "        #Image                                                                 #print(\"before anything\") #print(image.shape)\n",
    "        x = self.conv1(image)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2D(x)\n",
    "        \n",
    "        #print(\"before flatten:\" + str(x.shape))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"after flatten:\" + str(x.shape))\n",
    "\n",
    "        x = self.img_lin1(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "#         x = self.img_lin2(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        #Label                                                  #label = torch.unsqueeze(label, dim=1)\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        #label = self.dropout_label(label)\n",
    "        #coord = self.dropout_label(coord)\n",
    "        \n",
    "        y = torch.cat([label, coord], dim=1)\n",
    "#         y = self.dropout_label(y)\n",
    "        \n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "                                                                               #print(label) print(label.shape) print(coord) \n",
    "                                                                 #print(coord.shape) print(y) print(y.shape) #print(x.shape)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "        #print(\"after combination:\" + str(x.shape))\n",
    "#         x = self.bn1(concat)\n",
    "        x = self.comb_lin1(concat)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        \n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "#         self.img_lin1 = nn.Linear(z_dim, 256)\n",
    "        \n",
    "#         self.label_lin1 = nn.Linear(6,16)\n",
    "        \n",
    "#         self.coord_lin1 = nn.Linear(3,16)\n",
    "        \n",
    "        self.comb_lin1 = nn.Linear(z_dim+2+6, 1024)\n",
    "        self.comb_lin2 = nn.Linear(1024, init_filters*(conv_out_size**2))\n",
    "        \n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=init_filters, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_filters, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_pcent)\n",
    "        self.dropout2D = nn.Dropout2d(p=dropout_pcent)\n",
    "        \n",
    "        self.dropout_label = nn.Dropout(p=label_dropout_pcent)\n",
    "        \n",
    "    def forward(self, x, label, coord):\n",
    "        \n",
    "        #Latent Vector\n",
    "#         x = self.img_lin1(z)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "        #Label\n",
    "        label = F.one_hot(label, num_classes=6)\n",
    "        label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        #label = self.dropout_label(label)\n",
    "        #coord = self.dropout_label(coord)\n",
    "        \n",
    "        y = torch.cat([label, coord], dim=1)\n",
    "#         y = self.dropout_label(y)\n",
    "        \n",
    "        \n",
    "        #Concatenation\n",
    "        concat = torch.cat([x,y],dim=1)\n",
    "        #x = torch.cat([x,y],dim=1)\n",
    "        \n",
    "#         x = self.dropout(concat) if reintroduced change line below to x\n",
    "        x = self.comb_lin1(concat)\n",
    "        x = F.relu(x)\n",
    "        x = self.comb_lin2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_filters, conv_out_size, conv_out_size)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dec1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self, image, label, coord):\n",
    "        mu, log_var = self.encoder(image, label, coord)\n",
    "        \n",
    "        #print('mu: ', mu.shape)\n",
    "        #print('log_var: ', log_var.shape)\n",
    "        \n",
    "        #sample z from latent distribution q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu,std)\n",
    "        z = q.rsample()\n",
    "        #print('z shape: ', z.shape)\n",
    "        \n",
    "        reconstruction = self.decoder(z, label, coord)\n",
    "                \n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(mean, logscale, sample):\n",
    "    scale = torch.exp(logscale)\n",
    "    dist = torch.distributions.Normal(mean, scale)\n",
    "    log_pxz = dist.log_prob(sample)\n",
    "    return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "def kl_divergence(z, mu, std):\n",
    "    # --------------------------\n",
    "    # Monte carlo KL divergence\n",
    "    # --------------------------\n",
    "    # 1. define the first two probabilities (in this case Normal for both)\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    # 2. get the probabilities from the equation\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # kl\n",
    "    kl = (log_qzx - log_pz)\n",
    "    kl = kl.sum(-1)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        image, label, coord = batch\n",
    "        #print(image.size())\n",
    "        #print(label)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, log_var, z = model(image, label, coord)\n",
    "        \n",
    "        #print(reconstruction.shape)\n",
    "        \n",
    "        #image = image.to(torch.device('cpu'))\n",
    "#         recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "        recon_loss = loss(reconstruction, image)\n",
    "        \n",
    "        std = torch.exp(log_var / 2)\n",
    "        kl = kl_divergence(z, mu, std)\n",
    "        \n",
    "#         print(\"Training:\")\n",
    "#         print(\"kl:\" + str(kl))\n",
    "#         print(\"recon:\"+ str(recon_loss))\n",
    "\n",
    "        elbo = (beta*kl + recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "        \n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += elbo\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset) #Investigate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, epoch, object_relevant_z):\n",
    "    model.eval()\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_noisy = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            \n",
    "            image, label, coord = batch\n",
    "            \n",
    "#             noisy_z = torch.empty(batch_size, latent_dim).normal_(mean=0,std=1.0)\n",
    "            \n",
    "            noisy_z = []\n",
    "            for j in range (0, batch_size):\n",
    "                noisy_z.append(object_relevant_z[label[j].item()])\n",
    "#             noisy_z = torch.tensor(noisy_z).normal_(mean=0,std=1.0)\n",
    "            noisy_z = torch.tensor(noisy_z)\n",
    "                \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                coord = coord.to(device)\n",
    "                noisy_z = noisy_z.to(device)\n",
    "            \n",
    "            reconstruction_true, mu, log_var, z = model(image, label, coord)\n",
    "            \n",
    "            #print(\"Real Vector:\" + str(z.shape))\n",
    "            #print(\"Imitation Vector:\" + str(noisy_z.shape))\n",
    "            \n",
    "            reconstruction_noisy = model.decoder(noisy_z, label, coord)\n",
    "            \n",
    "            if (i == int(len(val_data)/dataloader.batch_size) - 1 and ( ((epoch%val_img_out_freq))==4) ): # or epoch > 90\n",
    "                num_rows = 4\n",
    "                both = torch.cat((image.view(batch_size, 3, image_size, image_size)[:4], \n",
    "                                  reconstruction_true.view(batch_size, 3, image_size, image_size)[:4],\n",
    "                                  reconstruction_noisy.view(batch_size, 3, image_size, image_size)[:4]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "            \n",
    "#             recon_loss = gaussian_likelihood(reconstruction, log_scale, image)\n",
    "            recon_loss_true = loss(reconstruction_true, image)\n",
    "            recon_loss_noisy = loss(reconstruction_noisy, image)\n",
    "        \n",
    "            std = torch.exp(log_var / 2)\n",
    "            kl = kl_divergence(z, mu, std)\n",
    "            \n",
    "#             print(\"Validation:\")\n",
    "#             print(\"kl:\" + str(kl))\n",
    "#             print(\"recon:\"+ str(recon_loss))\n",
    "\n",
    "            elbo_true = (beta*kl + recon_loss_true)\n",
    "            elbo_true = elbo_true.mean()\n",
    "        \n",
    "            elbo_noisy = (beta*kl + recon_loss_noisy)\n",
    "            elbo_noisy = elbo_noisy.mean()\n",
    "            \n",
    "            running_loss_true += elbo_true\n",
    "            \n",
    "            running_loss_noisy += elbo_noisy\n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    val_loss_true = running_loss_true/len(dataloader.dataset)\n",
    "    val_loss_noisy = running_loss_noisy/len(dataloader.dataset)\n",
    "    return val_loss_true, val_loss_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image, label, coord = batch\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            mu, logvar = model.encoder(image.cuda(), label.cuda(), coord.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "#         print(len(latent))\n",
    "#         print(latent)\n",
    "#         print(len(target))\n",
    "#         print(target)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    drop = 0.5\n",
    "    epochs_drop = 25.0\n",
    "    lrate = initial_learning_rate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def run_each():\n",
    "    train_loss = []\n",
    "    val_loss_true = []\n",
    "    val_loss_noisy = []\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        sleep(0.2)\n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        \n",
    "        latent, target = generate_latent_vectors(model, val_loader)\n",
    "        unique = set(target)\n",
    "        unique = list(unique)\n",
    "        object_relevant_z= dict()\n",
    "        \n",
    "        df = pd.DataFrame(list(zip(latent, target)), columns =['vector', 'object_type'])\n",
    "        for obj_type in unique:\n",
    "            object_relevant_z[obj_type] = df.loc[df['object_type'] == obj_type, 'vector'].mean()\n",
    "        \n",
    "        val_epoch_loss_true, val_epoch_loss_noisy = validate(model, val_loader, epoch, object_relevant_z)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss_true.append(val_epoch_loss_true)\n",
    "        val_loss_noisy.append(val_epoch_loss_noisy)\n",
    "        \n",
    "        learning_rate = step_decay(epoch)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#         print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    return train_loss, val_loss_true, val_loss_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    train_loss, val_loss_true, val_loss_noisy = run_each()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss_true, label=\"Validation Loss (True Reconstruction)\")\n",
    "    plt.plot(range(1,epochs+1), val_loss_noisy, label=\"Validation Loss (Noisy Reconstruction)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    latent, target = generate_latent_vectors(model, val_loader)\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/sample_latent_vectors'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Latent\", \"Target\"])\n",
    "        \n",
    "        for i in range (0,len(latent)):\n",
    "            latent[i] = list(latent[i])\n",
    "            \n",
    "        wr.writerows(zip(latent, target))\n",
    "    \n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        \n",
    "        for i in range (0,len(train_loss)):\n",
    "            train_loss[i] = train_loss[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss_true)):\n",
    "            val_loss_true[i] = val_loss_true[i].item()\n",
    "        \n",
    "        for i in range (0,len(val_loss_noisy)):\n",
    "            val_loss_noisy[i] = val_loss_noisy[i].item()\n",
    "            \n",
    "        wr.writerows(zip(train_loss, val_loss_true, val_loss_noisy))\n",
    "        \n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter,\n",
    "                     value,\n",
    "                     train_loss[-1],\n",
    "                     val_loss_true[-1],\n",
    "                     val_loss_noisy[-1],\n",
    "                     enc_out_dim, \n",
    "                     latent_dim, \n",
    "                     epochs,\n",
    "                     batch_size, \n",
    "                     initial_learning_rate, \n",
    "                     kernel_size, \n",
    "                     stride,\n",
    "                     padding, \n",
    "                     init_filters,\n",
    "                     dropout_pcent,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "    \"enc_out_dim\": 512,\n",
    "    \"latent_dim\": 128,\n",
    "    \"conv_out_size\": 4,\n",
    "    \n",
    "    \"epochs\" : 50,\n",
    "    \"batch_size\" : 8,\n",
    "    \"initial_learning_rate\" : 0.001,\n",
    "    \n",
    "    \"kernel_size\" : 5,\n",
    "    \"stride\" : 2,\n",
    "    \"padding\" : 2,\n",
    "    \n",
    "    \"init_filters\" : 128,\n",
    "    \n",
    "    \"dropout_pcent\": 0.0,\n",
    "    \"image_size\": 128,\n",
    "    \n",
    "    \"label_dropout_pcent\": 0.0\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/188 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9a7420496c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#Run the Test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mrunall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-a4fda657876b>\u001b[0m in \u001b[0;36mrunall\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrunall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss_noisy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_each\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Train Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Validation Loss (True Reconstruction)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1c68aaf03534>\u001b[0m in \u001b[0;36mrun_each\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch+1} of {epochs}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrain_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlatent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_latent_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9c66c5f3c03c>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mreconstruction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print(reconstruction.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a25c73b0db1c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image, label, coord)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print('mu: ', mu.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-91dd8f429995>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image, label, coord)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m#print(\"after combination:\" + str(x.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;31m#         x = self.bn1(concat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomb_lin1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "val_img_out_freq = 5\n",
    "parameter_list = ['ImageandCoordinateAdded']\n",
    "value_list = [0]\n",
    "\n",
    "for parameter in parameter_list:\n",
    "    for value in value_list:\n",
    "        #Update the values to be updated and rerun the experiment\n",
    "        enc_out_dim = param_dict[\"enc_out_dim\"]\n",
    "        latent_dim = param_dict[\"latent_dim\"]\n",
    "        conv_out_size = param_dict[\"conv_out_size\"]\n",
    "\n",
    "        epochs = param_dict[\"epochs\"]\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        initial_learning_rate = param_dict[\"initial_learning_rate\"]\n",
    "\n",
    "        kernel_size = param_dict[\"kernel_size\"]\n",
    "        stride = param_dict[\"stride\"]\n",
    "        padding = param_dict[\"padding\"]\n",
    "        init_filters = param_dict[\"init_filters\"]\n",
    "        \n",
    "        dropout_pcent = param_dict[\"dropout_pcent\"]\n",
    "        image_size = param_dict[\"image_size\"]\n",
    "        \n",
    "        label_dropout_pcent = param_dict[\"label_dropout_pcent\"]\n",
    "        \n",
    "        beta = 1\n",
    "        \n",
    "        #Move batch_size to before so its trained on the same split?\n",
    "        set_used = 'datasets/SmallGrey'\n",
    "        train_data = ActiveVisionDataset(csv_file=set_used+'/TrainSet/rgbCSV.csv', root_dir=set_used+'/TrainSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        val_data = ActiveVisionDataset(csv_file=set_used+'/ValSet/rgbCSV.csv', root_dir= set_used+'/ValSet/segImg/', transform = torchvision.transforms.ToTensor())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        img_size = len(train_data[0][0][0])\n",
    "        model = ConditionalVAE(latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate)\n",
    "\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0])).to(device)\n",
    "        loss = nn.MSELoss(reduction = 'sum')\n",
    "        \n",
    "        \n",
    "        #Change the value to a string for later\n",
    "        value = str(value)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "        os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "        \n",
    "        #Run the Test\n",
    "        runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
